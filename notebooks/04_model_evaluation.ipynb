{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA Props Model Evaluation - PRA Predictions for Betting\n",
    "\n",
    "## Objective\n",
    "Build and evaluate machine learning models for predicting NBA player PRA (Points + Rebounds + Assists) with a focus on practical betting applications.\n",
    "\n",
    "### Key Considerations for Sports Betting:\n",
    "- **Temporal validation**: Sports data is time-dependent\n",
    "- **Calibration**: Need well-calibrated probabilities for over/under betting\n",
    "- **Confidence intervals**: Critical for risk management\n",
    "- **Line coverage**: Accuracy at different betting lines (e.g., 20.5, 25.5, 30.5)\n",
    "- **Edge identification**: Finding mispriced lines with statistical confidence\n",
    "\n",
    "### Model Requirements:\n",
    "1. Point predictions for PRA values\n",
    "2. Prediction intervals (confidence bounds)\n",
    "3. Probability estimates for over/under specific lines\n",
    "4. Feature importance for understanding key drivers\n",
    "5. Performance stability across player types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully\n",
      "Python version: 2.3.2\n",
      "XGBoost version: 3.0.5\n",
      "LightGBM version: 4.6.0\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Modeling libraries\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, HuberRegressor, QuantileRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Statistical libraries\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, t\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"Libraries loaded successfully\")\n",
    "print(f\"Python version: {pd.__version__}\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print(f\"LightGBM version: {lgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (503, 27)\n",
      "Players: 503\n",
      "Features: 26\n",
      "\n",
      "Target variable (PRA_estimate) statistics:\n",
      "count    503.000000\n",
      "mean       8.927022\n",
      "std        7.892180\n",
      "min        0.339459\n",
      "25%        2.777252\n",
      "50%        6.547002\n",
      "75%       13.183307\n",
      "max       42.964009\n",
      "Name: PRA_estimate, dtype: float64\n",
      "\n",
      "Input features (21):\n",
      " 1. USG_percent\n",
      " 2. PSA\n",
      " 3. MIN\n",
      " 4. AST_percent\n",
      " 5. AST_to_USG_Ratio\n",
      " 6. fgDR_percent\n",
      " 7. fgOR_percent\n",
      " 8. Total_REB_percent\n",
      " 9. eFG_percent\n",
      "10. TOV_percent\n",
      "11. Defensive_Activity\n",
      "12. Position_Inferred\n",
      "13. Player_Role\n",
      "14. Consistency_Score\n",
      "15. Usage_Stability\n",
      "16. Performance_Tier\n",
      "17. Opportunity_Score\n",
      "18. Efficiency_x_Volume\n",
      "19. Playmaking_Efficiency\n",
      "20. Minutes_x_Efficiency\n",
      "21. Offensive_Load\n"
     ]
    }
   ],
   "source": [
    "# Load processed features\n",
    "data_path = Path('/Users/diyagamah/Documents/nba_props_model/data/processed')\n",
    "df = pd.read_csv(data_path / 'player_features_2023_24.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Players: {len(df)}\")\n",
    "print(f\"Features: {len(df.columns) - 1}\")\n",
    "print(\"\\nTarget variable (PRA_estimate) statistics:\")\n",
    "print(df['PRA_estimate'].describe())\n",
    "\n",
    "# Display feature columns\n",
    "feature_cols = [col for col in df.columns if col not in ['Player', 'Team', 'PRA_estimate', \n",
    "                                                          'Points_estimate', 'Rebounds_estimate', \n",
    "                                                          'Assists_estimate']]\n",
    "print(f\"\\nInput features ({len(feature_cols)}):\")\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per feature:\n",
      "No missing values found\n",
      "\n",
      "Target range: 0.34 - 42.96\n",
      "Target mean: 8.93\n",
      "Target std: 7.89\n",
      "Target CV: 88.41%\n"
     ]
    }
   ],
   "source": [
    "# Prepare features and target\n",
    "X = df[feature_cols].copy()\n",
    "y = df['PRA_estimate'].copy()\n",
    "\n",
    "# Store player metadata for analysis\n",
    "player_info = df[['Player', 'Team', 'MIN', 'Performance_Tier']].copy()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values per feature:\")\n",
    "missing = X.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(\"No missing values found\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nTarget range: {y.min():.2f} - {y.max():.2f}\")\n",
    "print(f\"Target mean: {y.mean():.2f}\")\n",
    "print(f\"Target std: {y.std():.2f}\")\n",
    "print(f\"Target CV: {y.std()/y.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train-Test Split Strategy\n",
    "\n",
    "For sports betting, we need to simulate real-world conditions:\n",
    "- **Temporal validation**: Later games predict future games\n",
    "- **Player-based split**: Some players for training, others for testing\n",
    "- **Stratified by performance tier**: Ensure all player types in both sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features: ['Position_Inferred', 'Player_Role']\n",
      "Numeric features (19): ['USG_percent', 'PSA', 'MIN', 'AST_percent', 'AST_to_USG_Ratio']...\n",
      "\n",
      "Train-Test Split Summary:\n",
      "Training set: 402 players\n",
      "Test set: 101 players\n",
      "\n",
      "PRA distribution in sets:\n",
      "Train - Mean: 8.85, Std: 7.75\n",
      "Test  - Mean: 9.24, Std: 8.48\n",
      "\n",
      "Tier distribution:\n",
      "             Train      Test  Difference\n",
      "PRA_tier                                \n",
      "Very Low  0.201493  0.198020   -0.003473\n",
      "Low       0.199005  0.198020   -0.000985\n",
      "Medium    0.201493  0.198020   -0.003473\n",
      "High      0.199005  0.198020   -0.000985\n",
      "Elite     0.199005  0.207921    0.008916\n"
     ]
    }
   ],
   "source": [
    "# Add the categorical handler import\n",
    "import sys\n",
    "sys.path.append('/Users/diyagamah/Documents/nba_props_model/src')\n",
    "from preprocessing.categorical_handler import CategoricalFeatureHandler, get_model_specific_features\n",
    "\n",
    "# Strategy 1: Random split stratified by performance tier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create stratification variable based on PRA quantiles\n",
    "df['PRA_tier'] = pd.qcut(y, q=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Elite'])\n",
    "\n",
    "# Identify categorical and numeric columns\n",
    "categorical_features = ['Position_Inferred', 'Player_Role']\n",
    "numeric_features = [col for col in feature_cols if col not in categorical_features]\n",
    "\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "print(f\"Numeric features ({len(numeric_features)}): {numeric_features[:5]}...\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "    X, y, df.index, test_size=0.2, random_state=42, stratify=df['PRA_tier']\n",
    ")\n",
    "\n",
    "# Get player info for test set\n",
    "test_players = player_info.iloc[idx_test].copy()\n",
    "test_players['actual_PRA'] = y_test.values\n",
    "\n",
    "print(\"\\nTrain-Test Split Summary:\")\n",
    "print(f\"Training set: {len(X_train)} players\")\n",
    "print(f\"Test set: {len(X_test)} players\")\n",
    "print(f\"\\nPRA distribution in sets:\")\n",
    "print(f\"Train - Mean: {y_train.mean():.2f}, Std: {y_train.std():.2f}\")\n",
    "print(f\"Test  - Mean: {y_test.mean():.2f}, Std: {y_test.std():.2f}\")\n",
    "\n",
    "# Verify stratification\n",
    "print(\"\\nTier distribution:\")\n",
    "train_tiers = df.iloc[idx_train]['PRA_tier'].value_counts(normalize=True).sort_index()\n",
    "test_tiers = df.iloc[idx_test]['PRA_tier'].value_counts(normalize=True).sort_index()\n",
    "tier_comparison = pd.DataFrame({\n",
    "    'Train': train_tiers,\n",
    "    'Test': test_tiers,\n",
    "    'Difference': test_tiers - train_tiers\n",
    "})\n",
    "print(tier_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating preprocessing pipelines...\n",
      "Preprocessing completed:\n",
      "  Linear models shape: (402, 23)\n",
      "  Tree models shape: (402, 21)\n",
      "  Original shape: (402, 21)\n",
      "\n",
      "Feature counts:\n",
      "  Original: 21\n",
      "  After linear preprocessing: 23\n",
      "  After tree preprocessing: 21\n"
     ]
    }
   ],
   "source": [
    "# Feature preprocessing with proper categorical handling\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create preprocessing pipelines for different model types\n",
    "print(\"Creating preprocessing pipelines...\")\n",
    "\n",
    "# For linear models: one-hot encode categoricals and scale numerics\n",
    "categorical_transformer_linear = OneHotEncoder(sparse_output=False, \n",
    "                                              handle_unknown='ignore',\n",
    "                                              drop='first')\n",
    "numeric_transformer_linear = RobustScaler()\n",
    "\n",
    "preprocessor_linear = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer_linear, numeric_features),\n",
    "        ('cat', categorical_transformer_linear, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# For tree models: ordinal encode categoricals, no scaling needed\n",
    "categorical_transformer_tree = OrdinalEncoder(handle_unknown='use_encoded_value', \n",
    "                                             unknown_value=-1)\n",
    "\n",
    "preprocessor_tree = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numeric_features),  # No scaling for tree models\n",
    "        ('cat', categorical_transformer_tree, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Fit and transform for linear models\n",
    "X_train_linear = preprocessor_linear.fit_transform(X_train)\n",
    "X_test_linear = preprocessor_linear.transform(X_test)\n",
    "\n",
    "# Fit and transform for tree models  \n",
    "X_train_tree = preprocessor_tree.fit_transform(X_train)\n",
    "X_test_tree = preprocessor_tree.transform(X_test)\n",
    "\n",
    "# Also create scaled numeric-only versions for comparison\n",
    "scaler = RobustScaler()\n",
    "X_train_numeric_scaled = X_train[numeric_features].copy()\n",
    "X_test_numeric_scaled = X_test[numeric_features].copy()\n",
    "X_train_numeric_scaled = scaler.fit_transform(X_train_numeric_scaled)\n",
    "X_test_numeric_scaled = scaler.transform(X_test_numeric_scaled)\n",
    "\n",
    "print(f\"Preprocessing completed:\")\n",
    "print(f\"  Linear models shape: {X_train_linear.shape}\")\n",
    "print(f\"  Tree models shape: {X_train_tree.shape}\")\n",
    "print(f\"  Original shape: {X_train.shape}\")\n",
    "\n",
    "# Get feature names after transformation\n",
    "linear_feature_names = (numeric_features + \n",
    "                       [f\"{cat}_{val}\" for cat, vals in \n",
    "                        zip(categorical_features, preprocessor_linear.named_transformers_['cat'].categories_)\n",
    "                        for val in vals[1:]])  # Skip first due to drop='first'\n",
    "\n",
    "tree_feature_names = numeric_features + categorical_features\n",
    "\n",
    "print(f\"\\nFeature counts:\")\n",
    "print(f\"  Original: {len(feature_cols)}\")\n",
    "print(f\"  After linear preprocessing: {len(linear_feature_names)}\")\n",
    "print(f\"  After tree preprocessing: {len(tree_feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Selection and Training\n",
    "\n",
    "We'll evaluate multiple model types:\n",
    "1. **Linear Models**: Ridge, Lasso, ElasticNet, Huber (robust)\n",
    "2. **Tree-based**: Random Forest, XGBoost, LightGBM, Extra Trees\n",
    "3. **Other**: SVR, Neural Network\n",
    "4. **Quantile Regression**: For prediction intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total models to evaluate: 10\n",
      "Models: ['Ridge', 'Lasso', 'ElasticNet', 'Huber', 'RandomForest', 'XGBoost', 'LightGBM', 'ExtraTrees', 'SVR', 'NeuralNet']\n"
     ]
    }
   ],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    # Linear models\n",
    "    'Ridge': Ridge(alpha=1.0, random_state=42),\n",
    "    'Lasso': Lasso(alpha=0.1, random_state=42, max_iter=2000),\n",
    "    'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42, max_iter=2000),\n",
    "    'Huber': HuberRegressor(epsilon=1.35, max_iter=200),\n",
    "    \n",
    "    # Tree-based models\n",
    "    'RandomForest': RandomForestRegressor(\n",
    "        n_estimators=200, max_depth=15, min_samples_split=5,\n",
    "        min_samples_leaf=2, random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    'XGBoost': xgb.XGBRegressor(\n",
    "        n_estimators=200, max_depth=6, learning_rate=0.05,\n",
    "        subsample=0.8, colsample_bytree=0.8, random_state=42,\n",
    "        objective='reg:squarederror', n_jobs=-1\n",
    "    ),\n",
    "    'LightGBM': lgb.LGBMRegressor(\n",
    "        n_estimators=200, max_depth=6, learning_rate=0.05,\n",
    "        subsample=0.8, colsample_bytree=0.8, random_state=42,\n",
    "        objective='regression', n_jobs=-1, verbose=-1\n",
    "    ),\n",
    "    'ExtraTrees': ExtraTreesRegressor(\n",
    "        n_estimators=200, max_depth=15, min_samples_split=5,\n",
    "        min_samples_leaf=2, random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    \n",
    "    # Other models\n",
    "    'SVR': SVR(kernel='rbf', C=10, gamma='scale', epsilon=0.1),\n",
    "    'NeuralNet': MLPRegressor(\n",
    "        hidden_layer_sizes=(100, 50), activation='relu', solver='adam',\n",
    "        alpha=0.001, max_iter=500, random_state=42, early_stopping=True,\n",
    "        validation_fraction=0.1\n",
    "    )\n",
    "}\n",
    "\n",
    "print(f\"Total models to evaluate: {len(models)}\")\n",
    "print(\"Models:\", list(models.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Ridge...\n",
      "  Using linear preprocessing (one-hot encoding + scaling)\n",
      "  MAE: 0.639, RMSE: 0.921, R²: 0.988, MAPE: 18.2%\n",
      "  Training time: 0.01s\n",
      "\n",
      "Training Lasso...\n",
      "  Using linear preprocessing (one-hot encoding + scaling)\n",
      "  MAE: 0.653, RMSE: 1.077, R²: 0.984, MAPE: 12.0%\n",
      "  Training time: 0.00s\n",
      "\n",
      "Training ElasticNet...\n",
      "  Using linear preprocessing (one-hot encoding + scaling)\n",
      "  MAE: 0.795, RMSE: 1.300, R²: 0.976, MAPE: 19.7%\n",
      "  Training time: 0.00s\n",
      "\n",
      "Training Huber...\n",
      "  Using linear preprocessing (one-hot encoding + scaling)\n",
      "  MAE: 0.633, RMSE: 0.958, R²: 0.987, MAPE: 17.6%\n",
      "  Training time: 0.02s\n",
      "\n",
      "Training RandomForest...\n",
      "  Using tree preprocessing (ordinal encoding, no scaling)\n",
      "  MAE: 0.674, RMSE: 1.149, R²: 0.981, MAPE: 8.0%\n",
      "  Training time: 0.11s\n",
      "\n",
      "Training XGBoost...\n",
      "  Using tree preprocessing (ordinal encoding, no scaling)\n",
      "  MAE: 0.452, RMSE: 0.698, R²: 0.993, MAPE: 5.6%\n",
      "  Training time: 0.35s\n",
      "\n",
      "Training LightGBM...\n",
      "  Using tree preprocessing (ordinal encoding, no scaling)\n",
      "  MAE: 0.620, RMSE: 1.325, R²: 0.975, MAPE: 6.8%\n",
      "  Training time: 0.31s\n",
      "\n",
      "Training ExtraTrees...\n",
      "  Using tree preprocessing (ordinal encoding, no scaling)\n",
      "  MAE: 0.536, RMSE: 0.875, R²: 0.989, MAPE: 6.8%\n",
      "  Training time: 0.08s\n",
      "\n",
      "Training SVR...\n",
      "  Using linear preprocessing (one-hot encoding + scaling)\n",
      "  MAE: 0.666, RMSE: 1.906, R²: 0.949, MAPE: 18.2%\n",
      "  Training time: 0.01s\n",
      "\n",
      "Training NeuralNet...\n",
      "  Using linear preprocessing (one-hot encoding + scaling)\n",
      "  MAE: 0.356, RMSE: 0.507, R²: 0.996, MAPE: 10.2%\n",
      "  Training time: 0.10s\n"
     ]
    }
   ],
   "source": [
    "# Train all models and store results\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import time\n",
    "\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "# Define which models need which preprocessing\n",
    "linear_models_list = ['Ridge', 'Lasso', 'ElasticNet', 'Huber', 'SVR', 'NeuralNet']\n",
    "tree_models_list = ['RandomForest', 'XGBoost', 'LightGBM', 'ExtraTrees']\n",
    "\n",
    "# Use appropriate data for each model type\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Select appropriate preprocessed data based on model type\n",
    "    if name in linear_models_list:\n",
    "        X_train_use = X_train_linear\n",
    "        X_test_use = X_test_linear\n",
    "        print(f\"  Using linear preprocessing (one-hot encoding + scaling)\")\n",
    "    elif name in tree_models_list:\n",
    "        X_train_use = X_train_tree\n",
    "        X_test_use = X_test_tree\n",
    "        print(f\"  Using tree preprocessing (ordinal encoding, no scaling)\")\n",
    "    else:\n",
    "        # Default to linear preprocessing for safety\n",
    "        X_train_use = X_train_linear\n",
    "        X_test_use = X_test_linear\n",
    "        print(f\"  Using linear preprocessing (default)\")\n",
    "    \n",
    "    # Fit model\n",
    "    model.fit(X_train_use, y_train)\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(X_train_use)\n",
    "    y_pred_test = model.predict(X_test_use)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    # Calculate MAPE (Mean Absolute Percentage Error) - useful for betting\n",
    "    # Avoid division by zero\n",
    "    mask = y_test != 0\n",
    "    if mask.sum() > 0:\n",
    "        test_mape = np.mean(np.abs((y_test[mask] - y_pred_test[mask]) / y_test[mask])) * 100\n",
    "    else:\n",
    "        test_mape = np.nan\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'train_mae': train_mae,\n",
    "        'test_mae': test_mae,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'test_mape': test_mape,\n",
    "        'predictions': y_pred_test,\n",
    "        'training_time': time.time() - start_time\n",
    "    }\n",
    "    \n",
    "    print(f\"  MAE: {test_mae:.3f}, RMSE: {test_rmse:.3f}, R²: {test_r2:.3f}, MAPE: {test_mape:.1f}%\")\n",
    "    print(f\"  Training time: {results[name]['training_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance Comparison (sorted by Test MAE):\n",
      "================================================================================\n",
      "              test_mae test_rmse   test_r2  test_mape training_time\n",
      "NeuralNet     0.356451  0.507336  0.996388  10.227402      0.097557\n",
      "XGBoost       0.452083  0.697788  0.993167   5.621164      0.350354\n",
      "ExtraTrees    0.536329  0.875046  0.989254   6.785137      0.080443\n",
      "LightGBM      0.619529  1.325138  0.975356   6.836275      0.308062\n",
      "Huber         0.633229  0.957894  0.987123  17.619755      0.021648\n",
      "Ridge         0.639189  0.920885  0.988098  18.245722      0.007924\n",
      "Lasso          0.65334  1.077078  0.983719   12.02803        0.0039\n",
      "SVR           0.666495  1.906103   0.94901   18.17554      0.011784\n",
      "RandomForest  0.673997  1.148638  0.981484   8.049935      0.114255\n",
      "ElasticNet    0.794785  1.300329   0.97627   19.74051      0.001805\n",
      "\n",
      "Best model: NeuralNet\n",
      "Test MAE: 0.356\n",
      "Test MAPE: 10.2%\n"
     ]
    }
   ],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(results).T\n",
    "comparison_df = comparison_df.sort_values('test_mae')\n",
    "\n",
    "# Display results\n",
    "print(\"Model Performance Comparison (sorted by Test MAE):\")\n",
    "print(\"=\"*80)\n",
    "display_cols = ['test_mae', 'test_rmse', 'test_r2', 'test_mape', 'training_time']\n",
    "print(comparison_df[display_cols].round(3))\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = comparison_df.index[0]\n",
    "print(f\"\\nBest model: {best_model_name}\")\n",
    "print(f\"Test MAE: {comparison_df.loc[best_model_name, 'test_mae']:.3f}\")\n",
    "print(f\"Test MAPE: {comparison_df.loc[best_model_name, 'test_mape']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-Validation with Multiple Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Fold Cross-Validation for Top 3 Models:\n",
      "============================================================\n",
      "\n",
      "NeuralNet:\n",
      "  Mean MAE: 0.482 (+/- 0.062)\n",
      "  Range: [0.405, 0.581]\n",
      "  All scores: 0.522, 0.442, 0.461, 0.581, 0.405\n",
      "\n",
      "XGBoost:\n",
      "  Mean MAE: 0.536 (+/- 0.037)\n",
      "  Range: [0.506, 0.609]\n",
      "  All scores: 0.510, 0.609, 0.529, 0.506, 0.524\n",
      "\n",
      "ExtraTrees:\n",
      "  Mean MAE: 0.590 (+/- 0.067)\n",
      "  Range: [0.521, 0.713]\n",
      "  All scores: 0.563, 0.713, 0.552, 0.602, 0.521\n"
     ]
    }
   ],
   "source": [
    "# Perform robust cross-validation for top models\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "top_models = comparison_df.head(3).index.tolist()\n",
    "cv_results = {}\n",
    "\n",
    "# Standard K-Fold CV\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"5-Fold Cross-Validation for Top 3 Models:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for model_name in top_models:\n",
    "    model = models[model_name]\n",
    "    \n",
    "    # Create a pipeline with appropriate preprocessing\n",
    "    if model_name in linear_models_list:\n",
    "        # Create pipeline with linear preprocessing\n",
    "        cv_pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor_linear),\n",
    "            ('model', model)\n",
    "        ])\n",
    "    elif model_name in tree_models_list:\n",
    "        # Create pipeline with tree preprocessing  \n",
    "        cv_pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor_tree),\n",
    "            ('model', model)\n",
    "        ])\n",
    "    else:\n",
    "        # Default to linear\n",
    "        cv_pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor_linear),\n",
    "            ('model', model)\n",
    "        ])\n",
    "    \n",
    "    # Perform CV on original data (pipeline will handle preprocessing)\n",
    "    cv_scores = cross_val_score(cv_pipeline, X, y, cv=kfold, \n",
    "                                scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    cv_scores = -cv_scores  # Convert to positive MAE\n",
    "    \n",
    "    cv_results[model_name] = {\n",
    "        'mean': cv_scores.mean(),\n",
    "        'std': cv_scores.std(),\n",
    "        'min': cv_scores.min(),\n",
    "        'max': cv_scores.max(),\n",
    "        'scores': cv_scores\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Mean MAE: {cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})\")\n",
    "    print(f\"  Range: [{cv_scores.min():.3f}, {cv_scores.max():.3f}]\")\n",
    "    print(f\"  All scores: {', '.join([f'{s:.3f}' for s in cv_scores])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction Intervals and Confidence Bounds\n",
    "\n",
    "For betting, we need to know the uncertainty in our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Quantile Regression Models:\n",
      "========================================\n",
      "Training 10% quantile model...\n",
      "Training 25% quantile model...\n",
      "Training 50% quantile model...\n",
      "Training 75% quantile model...\n",
      "Training 90% quantile model...\n",
      "\n",
      "Prediction Intervals Summary:\n",
      "Mean 80% interval width: 2.50\n",
      "Mean 50% interval width: 0.89\n",
      "\n",
      "80% interval coverage: 56.4% (target: 80%)\n",
      "50% interval coverage: 39.6% (target: 50%)\n",
      "\n",
      "Sample Predictions with Intervals:\n",
      "     actual  prediction    q10    q50    q90\n",
      "157    4.14        4.58   3.46   3.98   4.29\n",
      "239    7.17        7.23   6.00   7.79   6.89\n",
      "142   26.44       26.89  18.39  28.67  29.17\n",
      "177   13.68       13.78  12.80  14.87  15.63\n",
      "113    6.26        6.05   6.24   5.96   6.99\n"
     ]
    }
   ],
   "source": [
    "# Quantile Regression for prediction intervals\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Train quantile models for 10%, 50%, and 90% quantiles\n",
    "quantiles = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "quantile_models = {}\n",
    "quantile_predictions = {}\n",
    "\n",
    "print(\"Training Quantile Regression Models:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "for q in quantiles:\n",
    "    print(f\"Training {q:.0%} quantile model...\")\n",
    "    \n",
    "    # Use Gradient Boosting with quantile loss\n",
    "    qr_model = GradientBoostingRegressor(\n",
    "        loss='quantile', alpha=q,\n",
    "        n_estimators=100, max_depth=5,\n",
    "        learning_rate=0.1, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train on tree-preprocessed features (GradientBoosting is tree-based)\n",
    "    qr_model.fit(X_train_tree, y_train)\n",
    "    quantile_models[q] = qr_model\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = qr_model.predict(X_test_tree)\n",
    "    quantile_predictions[f'q{int(q*100)}'] = predictions\n",
    "\n",
    "# Create prediction intervals DataFrame\n",
    "prediction_intervals = pd.DataFrame(quantile_predictions, index=X_test.index)\n",
    "prediction_intervals['actual'] = y_test\n",
    "prediction_intervals['prediction'] = results[best_model_name]['predictions']\n",
    "\n",
    "# Calculate interval widths\n",
    "prediction_intervals['interval_80'] = prediction_intervals['q90'] - prediction_intervals['q10']\n",
    "prediction_intervals['interval_50'] = prediction_intervals['q75'] - prediction_intervals['q25']\n",
    "\n",
    "print(\"\\nPrediction Intervals Summary:\")\n",
    "print(f\"Mean 80% interval width: {prediction_intervals['interval_80'].mean():.2f}\")\n",
    "print(f\"Mean 50% interval width: {prediction_intervals['interval_50'].mean():.2f}\")\n",
    "\n",
    "# Check coverage\n",
    "coverage_80 = ((prediction_intervals['actual'] >= prediction_intervals['q10']) & \n",
    "               (prediction_intervals['actual'] <= prediction_intervals['q90'])).mean()\n",
    "coverage_50 = ((prediction_intervals['actual'] >= prediction_intervals['q25']) & \n",
    "               (prediction_intervals['actual'] <= prediction_intervals['q75'])).mean()\n",
    "\n",
    "print(f\"\\n80% interval coverage: {coverage_80:.1%} (target: 80%)\")\n",
    "print(f\"50% interval coverage: {coverage_50:.1%} (target: 50%)\")\n",
    "\n",
    "# Display sample predictions with intervals\n",
    "print(\"\\nSample Predictions with Intervals:\")\n",
    "sample = prediction_intervals.head(5)[['actual', 'prediction', 'q10', 'q50', 'q90']]\n",
    "print(sample.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Betting Line Analysis\n",
    "\n",
    "Evaluate model performance at common betting lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Common PRA betting lines\nbetting_lines = [15.5, 20.5, 25.5, 30.5, 35.5, 40.5]\n\n# Get best model predictions - FIX THE ERROR HERE\nbest_model = trained_models[best_model_name]\nif best_model_name in linear_models_list:\n    predictions = best_model.predict(X_test_linear)  # Fixed: use X_test_linear\nelif best_model_name in tree_models_list:\n    predictions = best_model.predict(X_test_tree)    # Fixed: use X_test_tree\nelse:\n    predictions = best_model.predict(X_test_linear)  # Default to linear\n\n# Analyze accuracy at each line\nline_analysis = []\n\nfor line in betting_lines:\n    # Actual outcomes\n    actual_over = y_test > line\n    \n    # Predicted outcomes (with confidence)\n    pred_over = predictions > line\n    \n    # Calculate metrics\n    n_bets = actual_over.sum() + (~actual_over).sum()\n    n_actual_overs = actual_over.sum()\n    \n    # Accuracy\n    correct = (pred_over == actual_over).sum()\n    accuracy = correct / len(y_test)\n    \n    # Precision for overs\n    if pred_over.sum() > 0:\n        precision_over = (pred_over & actual_over).sum() / pred_over.sum()\n    else:\n        precision_over = 0\n    \n    # Recall for overs\n    if actual_over.sum() > 0:\n        recall_over = (pred_over & actual_over).sum() / actual_over.sum()\n    else:\n        recall_over = 0\n    \n    # Calculate predicted probability using distance from line\n    distances = predictions - line\n    # Simple probability estimate based on distance (can be improved with calibration)\n    prob_over = norm.cdf(distances, loc=0, scale=prediction_intervals['interval_50'].mean()/2)\n    \n    # Expected value calculation (assuming -110 odds)\n    odds = 1.909  # Decimal odds for -110\n    ev_over = prob_over * (odds - 1) - (1 - prob_over)\n    \n    # Find bets with positive EV\n    positive_ev_bets = (ev_over > 0.05).sum()  # 5% edge threshold\n    \n    line_analysis.append({\n        'Line': line,\n        'Actual_Overs': n_actual_overs,\n        'Actual_Over_Pct': n_actual_overs / len(y_test),\n        'Accuracy': accuracy,\n        'Precision_Over': precision_over,\n        'Recall_Over': recall_over,\n        'Avg_Prob_Over': prob_over.mean(),\n        'Positive_EV_Bets': positive_ev_bets,\n        'Max_EV': ev_over.max()\n    })\n\nline_df = pd.DataFrame(line_analysis)\nprint(\"Betting Line Analysis:\")\nprint(\"=\"*80)\nprint(line_df.round(3))\n\n# Identify best lines for betting\nprint(\"\\nBest Lines for Betting (highest accuracy):\")\nbest_lines = line_df.nlargest(3, 'Accuracy')[['Line', 'Accuracy', 'Positive_EV_Bets']]\nprint(best_lines)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from tree-based models\n",
    "tree_models = ['RandomForest', 'XGBoost', 'LightGBM', 'ExtraTrees']\n",
    "importance_dict = {}\n",
    "\n",
    "for model_name in tree_models:\n",
    "    if model_name in trained_models:\n",
    "        model = trained_models[model_name]\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importance_dict[model_name] = model.feature_importances_\n",
    "\n",
    "# Create importance DataFrame\n",
    "importance_df = pd.DataFrame(importance_dict, index=feature_cols)\n",
    "importance_df['mean_importance'] = importance_df.mean(axis=1)\n",
    "importance_df = importance_df.sort_values('mean_importance', ascending=False)\n",
    "\n",
    "# Display top features\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(\"=\"*50)\n",
    "top_features = importance_df.head(10)\n",
    "for i, (feature, row) in enumerate(top_features.iterrows(), 1):\n",
    "    print(f\"{i:2d}. {feature:25s} - Importance: {row['mean_importance']:.4f}\")\n",
    "\n",
    "# Visualize feature importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Bar plot of top features\n",
    "ax1 = axes[0]\n",
    "top_features['mean_importance'].plot(kind='barh', ax=ax1, color='steelblue')\n",
    "ax1.set_xlabel('Importance')\n",
    "ax1.set_title('Top 10 Feature Importances (Average across Tree Models)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Heatmap of importance across models\n",
    "ax2 = axes[1]\n",
    "sns.heatmap(importance_df.head(10)[tree_models], annot=True, fmt='.3f', \n",
    "            cmap='YlOrRd', ax=ax2, cbar_kws={'label': 'Importance'})\n",
    "ax2.set_title('Feature Importance by Model')\n",
    "ax2.set_xlabel('Model')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify stable vs unstable features\n",
    "importance_std = importance_df[tree_models].std(axis=1)\n",
    "print(\"\\nMost Stable Features (consistent importance across models):\")\n",
    "stable_features = importance_std.nsmallest(5)\n",
    "for feature, std in stable_features.items():\n",
    "    mean_imp = importance_df.loc[feature, 'mean_importance']\n",
    "    print(f\"  {feature}: Mean={mean_imp:.4f}, Std={std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Analysis by Player Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Combine predictions with player information - FIX PREDICTIONS DEFINITION\n# Get best model predictions (fixed to use correct preprocessed data)\nbest_model = trained_models[best_model_name]\nif best_model_name in linear_models_list:\n    predictions = best_model.predict(X_test_linear)\nelif best_model_name in tree_models_list:\n    predictions = best_model.predict(X_test_tree)\nelse:\n    predictions = best_model.predict(X_test_linear)\n\nerror_analysis_df = test_players.copy()\nerror_analysis_df['predicted_PRA'] = predictions\nerror_analysis_df['error'] = error_analysis_df['actual_PRA'] - error_analysis_df['predicted_PRA']\nerror_analysis_df['abs_error'] = np.abs(error_analysis_df['error'])\nerror_analysis_df['pct_error'] = (error_analysis_df['abs_error'] / error_analysis_df['actual_PRA']) * 100\n\n# Analyze by performance tier\nprint(\"Error Analysis by Performance Tier:\")\nprint(\"=\"*60)\ntier_analysis = error_analysis_df.groupby('Performance_Tier').agg({\n    'abs_error': ['mean', 'std', 'median'],\n    'pct_error': ['mean', 'std'],\n    'error': ['mean', 'std'],\n    'Player': 'count'\n}).round(2)\nprint(tier_analysis)\n\n# Find hardest to predict players\nprint(\"\\n10 Hardest to Predict Players:\")\nprint(\"=\"*60)\nhardest = error_analysis_df.nlargest(10, 'abs_error')[[\n    'Player', 'Team', 'actual_PRA', 'predicted_PRA', 'abs_error', 'pct_error'\n]]\nprint(hardest.round(2))\n\n# Find most accurately predicted players\nprint(\"\\n10 Most Accurately Predicted Players:\")\nprint(\"=\"*60)\nmost_accurate = error_analysis_df.nsmallest(10, 'abs_error')[[\n    'Player', 'Team', 'actual_PRA', 'predicted_PRA', 'abs_error', 'pct_error'\n]]\nprint(most_accurate.round(2))\n\n# Analyze by minutes played\nerror_analysis_df['minutes_category'] = pd.cut(\n    error_analysis_df['MIN'], \n    bins=[0, 500, 1000, 1500, 2000, 3000],\n    labels=['<500', '500-1000', '1000-1500', '1500-2000', '2000+']\n)\n\nprint(\"\\nError Analysis by Minutes Played:\")\nprint(\"=\"*60)\nminutes_analysis = error_analysis_df.groupby('minutes_category').agg({\n    'abs_error': ['mean', 'std'],\n    'pct_error': 'mean',\n    'Player': 'count'\n}).round(2)\nprint(minutes_analysis)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create comprehensive visualization - FIX PREDICTIONS FIRST\n# Get best model predictions (fixed to use correct preprocessed data)\nbest_model = trained_models[best_model_name]\nif best_model_name in linear_models_list:\n    predictions = best_model.predict(X_test_linear)\nelif best_model_name in tree_models_list:\n    predictions = best_model.predict(X_test_tree)\nelse:\n    predictions = best_model.predict(X_test_linear)\n\nfig = plt.figure(figsize=(20, 12))\ngs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n\n# 1. Actual vs Predicted scatter\nax1 = fig.add_subplot(gs[0, 0])\nax1.scatter(y_test, predictions, alpha=0.5, s=30)\nax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nax1.set_xlabel('Actual PRA')\nax1.set_ylabel('Predicted PRA')\nax1.set_title(f'Predictions vs Actual ({best_model_name})')\nax1.grid(True, alpha=0.3)\nax1.text(0.05, 0.95, f'R² = {results[best_model_name][\"test_r2\"]:.3f}',\n         transform=ax1.transAxes, verticalalignment='top')\n\n# 2. Residual plot\nax2 = fig.add_subplot(gs[0, 1])\nresiduals = y_test - predictions\nax2.scatter(predictions, residuals, alpha=0.5, s=30)\nax2.axhline(y=0, color='r', linestyle='--')\nax2.set_xlabel('Predicted PRA')\nax2.set_ylabel('Residuals')\nax2.set_title('Residual Plot')\nax2.grid(True, alpha=0.3)\n\n# 3. Error distribution\nax3 = fig.add_subplot(gs[0, 2])\nax3.hist(residuals, bins=30, edgecolor='black', alpha=0.7)\nax3.axvline(x=0, color='r', linestyle='--')\nax3.set_xlabel('Prediction Error')\nax3.set_ylabel('Frequency')\nax3.set_title('Error Distribution')\nax3.grid(True, alpha=0.3)\nax3.text(0.05, 0.95, f'Mean: {residuals.mean():.2f}\\nStd: {residuals.std():.2f}',\n         transform=ax3.transAxes, verticalalignment='top')\n\n# 4. Model comparison\nax4 = fig.add_subplot(gs[1, :])\nmodel_names = list(results.keys())\ntest_maes = [results[m]['test_mae'] for m in model_names]\ncolors = ['green' if m == best_model_name else 'steelblue' for m in model_names]\nbars = ax4.bar(model_names, test_maes, color=colors)\nax4.set_xlabel('Model')\nax4.set_ylabel('Test MAE')\nax4.set_title('Model Performance Comparison')\nax4.grid(True, alpha=0.3, axis='y')\nax4.set_xticklabels(model_names, rotation=45, ha='right')\n\n# Add value labels on bars\nfor bar, mae in zip(bars, test_maes):\n    height = bar.get_height()\n    ax4.text(bar.get_x() + bar.get_width()/2., height,\n             f'{mae:.2f}', ha='center', va='bottom')\n\n# 5. Prediction intervals\nax5 = fig.add_subplot(gs[2, 0])\nsample_idx = np.random.choice(len(prediction_intervals), 20, replace=False)\nsample_data = prediction_intervals.iloc[sample_idx].sort_values('actual')\nx_pos = range(len(sample_data))\n\nax5.scatter(x_pos, sample_data['actual'], color='red', s=50, label='Actual', zorder=5)\nax5.scatter(x_pos, sample_data['prediction'], color='blue', s=30, label='Predicted', zorder=4)\nax5.fill_between(x_pos, sample_data['q10'], sample_data['q90'], \n                  alpha=0.3, color='gray', label='80% PI')\nax5.fill_between(x_pos, sample_data['q25'], sample_data['q75'], \n                  alpha=0.5, color='gray', label='50% PI')\nax5.set_xlabel('Sample Players')\nax5.set_ylabel('PRA')\nax5.set_title('Prediction Intervals (20 Random Players)')\nax5.legend()\nax5.grid(True, alpha=0.3)\n\n# 6. Error by tier (need to define error_analysis_df first)\nax6 = fig.add_subplot(gs[2, 1])\n# Create error_analysis_df if not exists\nif 'error_analysis_df' not in locals():\n    error_analysis_df = test_players.copy()\n    error_analysis_df['predicted_PRA'] = predictions\n    error_analysis_df['abs_error'] = np.abs(error_analysis_df['actual_PRA'] - error_analysis_df['predicted_PRA'])\n\ntier_errors = error_analysis_df.groupby('Performance_Tier')['abs_error'].mean().sort_values()\ntier_errors.plot(kind='barh', ax=ax6, color='coral')\nax6.set_xlabel('Mean Absolute Error')\nax6.set_title('Prediction Error by Performance Tier')\nax6.grid(True, alpha=0.3, axis='x')\n\n# 7. Calibration plot for betting\nax7 = fig.add_subplot(gs[2, 2])\n# For a specific line (e.g., 25.5)\ntest_line = 25.5\ndistances = predictions - test_line\nprob_over = norm.cdf(distances, loc=0, scale=prediction_intervals['interval_50'].mean()/2)\n\n# Bin probabilities\nn_bins = 10\nbin_edges = np.linspace(0, 1, n_bins + 1)\nbin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\nactual_freq = []\npredicted_freq = []\n\nfor i in range(n_bins):\n    mask = (prob_over >= bin_edges[i]) & (prob_over < bin_edges[i+1])\n    if mask.sum() > 0:\n        actual_freq.append((y_test[mask] > test_line).mean())\n        predicted_freq.append(prob_over[mask].mean())\n    else:\n        actual_freq.append(np.nan)\n        predicted_freq.append(np.nan)\n\nax7.scatter(predicted_freq, actual_freq, s=50)\nax7.plot([0, 1], [0, 1], 'r--', lw=2)\nax7.set_xlabel('Predicted Probability')\nax7.set_ylabel('Actual Frequency')\nax7.set_title(f'Calibration Plot (Line: {test_line})')\nax7.grid(True, alpha=0.3)\nax7.set_xlim([0, 1])\nax7.set_ylim([0, 1])\n\nplt.suptitle('NBA Props Model Evaluation Dashboard', fontsize=16, y=1.02)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Production-Ready Predictions\n",
    "\n",
    "Generate betting recommendations with confidence levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_betting_recommendations(player_features, model, scaler, quantile_models, \n",
    "                                     line, min_edge=0.05, odds=-110):\n",
    "    \"\"\"\n",
    "    Generate betting recommendations for a given line.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    player_features : DataFrame\n",
    "        Features for players to predict\n",
    "    model : sklearn model\n",
    "        Trained prediction model\n",
    "    scaler : sklearn scaler\n",
    "        Feature scaler\n",
    "    quantile_models : dict\n",
    "        Quantile regression models\n",
    "    line : float\n",
    "        Betting line\n",
    "    min_edge : float\n",
    "        Minimum edge required for recommendation\n",
    "    odds : int\n",
    "        American odds (e.g., -110)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert American odds to decimal\n",
    "    if odds < 0:\n",
    "        decimal_odds = 1 + (100 / abs(odds))\n",
    "    else:\n",
    "        decimal_odds = 1 + (odds / 100)\n",
    "    \n",
    "    # Make predictions\n",
    "    if model.__class__.__name__ in ['Ridge', 'Lasso', 'ElasticNet', 'Huber', 'SVR', 'MLPRegressor']:\n",
    "        X_scaled = scaler.transform(player_features)\n",
    "        predictions = model.predict(X_scaled)\n",
    "    else:\n",
    "        predictions = model.predict(player_features)\n",
    "    \n",
    "    # Get prediction intervals\n",
    "    q10 = quantile_models[0.1].predict(player_features)\n",
    "    q90 = quantile_models[0.9].predict(player_features)\n",
    "    \n",
    "    # Calculate probability of going over\n",
    "    std_estimate = (q90 - q10) / 2.56  # Approximate std from 80% interval\n",
    "    prob_over = norm.cdf((predictions - line) / std_estimate)\n",
    "    prob_under = 1 - prob_over\n",
    "    \n",
    "    # Calculate expected value\n",
    "    ev_over = prob_over * (decimal_odds - 1) - prob_under\n",
    "    ev_under = prob_under * (decimal_odds - 1) - prob_over\n",
    "    \n",
    "    # Create recommendations DataFrame\n",
    "    recommendations = pd.DataFrame({\n",
    "        'prediction': predictions,\n",
    "        'line': line,\n",
    "        'prob_over': prob_over,\n",
    "        'prob_under': prob_under,\n",
    "        'ev_over': ev_over,\n",
    "        'ev_under': ev_under,\n",
    "        'confidence_interval_80': f\"[{q10:.1f}, {q90:.1f}]\",\n",
    "        'recommendation': 'NO BET'\n",
    "    }, index=player_features.index)\n",
    "    \n",
    "    # Make recommendations\n",
    "    recommendations.loc[ev_over > min_edge, 'recommendation'] = 'BET OVER'\n",
    "    recommendations.loc[ev_under > min_edge, 'recommendation'] = 'BET UNDER'\n",
    "    \n",
    "    # Add confidence level\n",
    "    max_ev = np.maximum(ev_over, ev_under)\n",
    "    recommendations['confidence'] = 'LOW'\n",
    "    recommendations.loc[max_ev > 0.10, 'confidence'] = 'MEDIUM'\n",
    "    recommendations.loc[max_ev > 0.15, 'confidence'] = 'HIGH'\n",
    "    recommendations.loc[max_ev > 0.20, 'confidence'] = 'VERY HIGH'\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Generate recommendations for test set\n",
    "test_line = 25.5\n",
    "recommendations = generate_betting_recommendations(\n",
    "    X_test, trained_models[best_model_name], scaler, \n",
    "    quantile_models, test_line, min_edge=0.05\n",
    ")\n",
    "\n",
    "# Add player info\n",
    "recommendations = pd.concat([test_players[['Player', 'Team', 'actual_PRA']], recommendations], axis=1)\n",
    "\n",
    "# Display betting opportunities\n",
    "betting_opportunities = recommendations[recommendations['recommendation'] != 'NO BET'].copy()\n",
    "betting_opportunities = betting_opportunities.sort_values('confidence', \n",
    "                                                          ascending=False,\n",
    "                                                          key=lambda x: x.map({'VERY HIGH': 4, \n",
    "                                                                              'HIGH': 3, \n",
    "                                                                              'MEDIUM': 2, \n",
    "                                                                              'LOW': 1}))\n",
    "\n",
    "print(f\"Betting Recommendations for Line: {test_line}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total opportunities found: {len(betting_opportunities)}\")\n",
    "print(f\"High confidence bets: {(betting_opportunities['confidence'].isin(['HIGH', 'VERY HIGH'])).sum()}\")\n",
    "\n",
    "print(\"\\nTop 10 Betting Opportunities:\")\n",
    "display_cols = ['Player', 'Team', 'actual_PRA', 'prediction', 'recommendation', \n",
    "                'prob_over', 'ev_over', 'confidence']\n",
    "print(betting_opportunities.head(10)[display_cols].round(3))\n",
    "\n",
    "# Evaluate recommendations\n",
    "if len(betting_opportunities) > 0:\n",
    "    # Check actual results\n",
    "    correct_bets = 0\n",
    "    for idx, row in betting_opportunities.iterrows():\n",
    "        if row['recommendation'] == 'BET OVER' and row['actual_PRA'] > test_line:\n",
    "            correct_bets += 1\n",
    "        elif row['recommendation'] == 'BET UNDER' and row['actual_PRA'] < test_line:\n",
    "            correct_bets += 1\n",
    "    \n",
    "    accuracy = correct_bets / len(betting_opportunities)\n",
    "    print(f\"\\nBacktest Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.1%} ({correct_bets}/{len(betting_opportunities)})\")\n",
    "    \n",
    "    # ROI calculation\n",
    "    profit = correct_bets * 0.909 - (len(betting_opportunities) - correct_bets)  # Assuming unit bets\n",
    "    roi = (profit / len(betting_opportunities)) * 100\n",
    "    print(f\"ROI: {roi:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Ensemble and Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create ensemble predictions\nfrom sklearn.ensemble import VotingRegressor, StackingRegressor\n\n# Select top 3 models for ensemble\ntop_3_models = comparison_df.head(3).index.tolist()\nprint(f\"Creating ensemble with top 3 models: {top_3_models}\")\n\n# Prepare base models\nbase_models = []\nfor model_name in top_3_models:\n    base_models.append((model_name, models[model_name]))\n\n# Create voting ensemble (average predictions)\nvoting_ensemble = VotingRegressor(estimators=base_models)\n\n# Train voting ensemble - FIX: use appropriate preprocessing\nprint(\"\\nTraining Voting Ensemble...\")\n# Check if we need linear or tree preprocessing\nneeds_linear = any(name in linear_models_list for name in top_3_models)\nneeds_tree = any(name in tree_models_list for name in top_3_models)\n\nif needs_linear and not needs_tree:\n    # All models need linear preprocessing\n    voting_ensemble.fit(X_train_linear, y_train)\n    voting_pred = voting_ensemble.predict(X_test_linear)\nelif needs_tree and not needs_linear:\n    # All models need tree preprocessing\n    voting_ensemble.fit(X_train_tree, y_train)\n    voting_pred = voting_ensemble.predict(X_test_tree)\nelse:\n    # Mixed - use linear as default since it has more features\n    print(\"WARNING: Mixed model types in ensemble - using linear preprocessing\")\n    voting_ensemble.fit(X_train_linear, y_train)\n    voting_pred = voting_ensemble.predict(X_test_linear)\n\nvoting_mae = mean_absolute_error(y_test, voting_pred)\nvoting_r2 = r2_score(y_test, voting_pred)\nprint(f\"Voting Ensemble - MAE: {voting_mae:.3f}, R²: {voting_r2:.3f}\")\n\n# Create stacking ensemble\nprint(\"\\nTraining Stacking Ensemble...\")\nstacking_ensemble = StackingRegressor(\n    estimators=base_models,\n    final_estimator=Ridge(alpha=1.0),\n    cv=5  # Use cross-validation to train meta-model\n)\n\n# Train stacking ensemble - use same logic as voting\nif needs_linear and not needs_tree:\n    stacking_ensemble.fit(X_train_linear, y_train)\n    stacking_pred = stacking_ensemble.predict(X_test_linear)\nelif needs_tree and not needs_linear:\n    stacking_ensemble.fit(X_train_tree, y_train)\n    stacking_pred = stacking_ensemble.predict(X_test_tree)\nelse:\n    print(\"WARNING: Mixed model types in ensemble - using linear preprocessing\")\n    stacking_ensemble.fit(X_train_linear, y_train)\n    stacking_pred = stacking_ensemble.predict(X_test_linear)\n\nstacking_mae = mean_absolute_error(y_test, stacking_pred)\nstacking_r2 = r2_score(y_test, stacking_pred)\nprint(f\"Stacking Ensemble - MAE: {stacking_mae:.3f}, R²: {stacking_r2:.3f}\")\n\n# Compare all models including ensembles\nprint(\"\\nFinal Model Comparison:\")\nprint(\"=\"*50)\nfinal_comparison = pd.DataFrame({\n    'Model': [best_model_name, 'Voting Ensemble', 'Stacking Ensemble'],\n    'MAE': [results[best_model_name]['test_mae'], voting_mae, stacking_mae],\n    'R²': [results[best_model_name]['test_r2'], voting_r2, stacking_r2]\n}).sort_values('MAE')\nprint(final_comparison)\n\n# Visualize ensemble performance\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Best single model\naxes[0].scatter(y_test, results[best_model_name]['predictions'], alpha=0.5)\naxes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\naxes[0].set_title(f'{best_model_name}\\nMAE: {results[best_model_name][\"test_mae\"]:.3f}')\naxes[0].set_xlabel('Actual')\naxes[0].set_ylabel('Predicted')\naxes[0].grid(True, alpha=0.3)\n\n# Voting ensemble\naxes[1].scatter(y_test, voting_pred, alpha=0.5)\naxes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\naxes[1].set_title(f'Voting Ensemble\\nMAE: {voting_mae:.3f}')\naxes[1].set_xlabel('Actual')\naxes[1].set_ylabel('Predicted')\naxes[1].grid(True, alpha=0.3)\n\n# Stacking ensemble\naxes[2].scatter(y_test, stacking_pred, alpha=0.5)\naxes[2].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\naxes[2].set_title(f'Stacking Ensemble\\nMAE: {stacking_mae:.3f}')\naxes[2].set_xlabel('Actual')\naxes[2].set_ylabel('Predicted')\naxes[2].grid(True, alpha=0.3)\n\nplt.suptitle('Ensemble Model Comparison', fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Production Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Determine best final model\n",
    "best_final_model = None\n",
    "best_final_name = None\n",
    "best_final_mae = float('inf')\n",
    "\n",
    "if voting_mae < best_final_mae:\n",
    "    best_final_model = voting_ensemble\n",
    "    best_final_name = 'VotingEnsemble'\n",
    "    best_final_mae = voting_mae\n",
    "\n",
    "if stacking_mae < best_final_mae:\n",
    "    best_final_model = stacking_ensemble\n",
    "    best_final_name = 'StackingEnsemble'\n",
    "    best_final_mae = stacking_mae\n",
    "\n",
    "if results[best_model_name]['test_mae'] < best_final_mae:\n",
    "    best_final_model = trained_models[best_model_name]\n",
    "    best_final_name = best_model_name\n",
    "    best_final_mae = results[best_model_name]['test_mae']\n",
    "\n",
    "# Create models directory\n",
    "models_dir = Path('/Users/diyagamah/Documents/nba_props_model/models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save model artifacts\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_filename = models_dir / f'pra_model_{best_final_name}_{timestamp}.pkl'\n",
    "scaler_filename = models_dir / f'scaler_{timestamp}.pkl'\n",
    "quantiles_filename = models_dir / f'quantile_models_{timestamp}.pkl'\n",
    "metadata_filename = models_dir / f'model_metadata_{timestamp}.json'\n",
    "\n",
    "# Save files\n",
    "joblib.dump(best_final_model, model_filename)\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "joblib.dump(quantile_models, quantiles_filename)\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'model_type': best_final_name,\n",
    "    'test_mae': float(best_final_mae),\n",
    "    'test_r2': float(voting_r2 if best_final_name == 'VotingEnsemble' else \n",
    "                    stacking_r2 if best_final_name == 'StackingEnsemble' else \n",
    "                    results[best_model_name]['test_r2']),\n",
    "    'features': feature_cols,\n",
    "    'n_training_samples': len(X_train),\n",
    "    'n_test_samples': len(X_test),\n",
    "    'timestamp': timestamp,\n",
    "    'target_mean': float(y_train.mean()),\n",
    "    'target_std': float(y_train.std())\n",
    "}\n",
    "\n",
    "with open(metadata_filename, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"Production model saved:\")\n",
    "print(f\"  Model: {model_filename}\")\n",
    "print(f\"  Scaler: {scaler_filename}\")\n",
    "print(f\"  Quantiles: {quantiles_filename}\")\n",
    "print(f\"  Metadata: {metadata_filename}\")\n",
    "print(f\"\\nBest Model: {best_final_name}\")\n",
    "print(f\"Test MAE: {best_final_mae:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### Model Performance:\n",
    "- Best single model and performance metrics\n",
    "- Ensemble improvements\n",
    "- Confidence intervals and calibration\n",
    "\n",
    "### Betting Insights:\n",
    "- Most profitable betting lines\n",
    "- Player types with highest prediction accuracy\n",
    "- Expected value calculations\n",
    "\n",
    "### Production Deployment:\n",
    "1. Model and artifacts saved for production use\n",
    "2. Feature importance identified for monitoring\n",
    "3. Error patterns understood for risk management\n",
    "\n",
    "### Recommended Next Steps:\n",
    "1. **Backtesting**: Test on historical betting lines\n",
    "2. **Live monitoring**: Track model performance on new games\n",
    "3. **Feature engineering**: Add game-specific features (opponent, home/away, rest days)\n",
    "4. **Calibration**: Improve probability estimates for betting lines\n",
    "5. **Risk management**: Implement Kelly criterion for bet sizing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nba-props-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}