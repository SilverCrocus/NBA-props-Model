{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA Props Model - Feature Engineering Pipeline\n",
    "\n",
    "This notebook implements the three-tier feature architecture for NBA player PRA (Points + Rebounds + Assists) prediction.\n",
    "\n",
    "## Feature Tiers:\n",
    "1. **Core Performance Engine**: Player's baseline abilities (USG%, PSA, AST%, Rebounding)\n",
    "2. **Contextual Modulators**: Game-specific factors (Minutes, Opponent, Rest)\n",
    "3. **Temporal Dynamics**: Recent form and trends (Rolling averages, EWMA, Volatility)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# For feature engineering\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTGDataLoader:\n",
    "    \"\"\"Load and process CTG data from organized directory structure\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path='/Users/diyagamah/Documents/nba_props_model/data'):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.player_data_path = self.base_path / 'ctg_data_organized' / 'players'\n",
    "        self.team_data_path = self.base_path / 'ctg_team_data'\n",
    "        \n",
    "    def load_player_season_data(self, season='2023-24', season_type='regular_season'):\n",
    "        \"\"\"Load all player data for a specific season\"\"\"\n",
    "        season_path = self.player_data_path / season / season_type\n",
    "        \n",
    "        if not season_path.exists():\n",
    "            print(f\"Path not found: {season_path}\")\n",
    "            return None\n",
    "            \n",
    "        data_dict = {}\n",
    "        \n",
    "        # Load offensive overview\n",
    "        offensive_path = season_path / 'offensive_overview.csv'\n",
    "        if offensive_path.exists():\n",
    "            data_dict['offensive'] = pd.read_csv(offensive_path)\n",
    "            print(f\"Loaded offensive_overview: {len(data_dict['offensive'])} players\")\n",
    "        \n",
    "        # Load defense and rebounding\n",
    "        defense_path = season_path / 'defense_rebounding.csv'\n",
    "        if defense_path.exists():\n",
    "            data_dict['defense'] = pd.read_csv(defense_path)\n",
    "            print(f\"Loaded defense_rebounding: {len(data_dict['defense'])} players\")\n",
    "            \n",
    "        # Load shooting data\n",
    "        shooting_overall_path = season_path / 'shooting_overall.csv'\n",
    "        if shooting_overall_path.exists():\n",
    "            data_dict['shooting'] = pd.read_csv(shooting_overall_path)\n",
    "            print(f\"Loaded shooting_overall: {len(data_dict['shooting'])} players\")\n",
    "            \n",
    "        # Load shooting accuracy\n",
    "        shooting_accuracy_path = season_path / 'shooting_accuracy.csv'\n",
    "        if shooting_accuracy_path.exists():\n",
    "            data_dict['shooting_accuracy'] = pd.read_csv(shooting_accuracy_path)\n",
    "            print(f\"Loaded shooting_accuracy: {len(data_dict['shooting_accuracy'])} players\")\n",
    "            \n",
    "        # Load foul drawing\n",
    "        foul_path = season_path / 'foul_drawing.csv'\n",
    "        if foul_path.exists():\n",
    "            data_dict['fouls'] = pd.read_csv(foul_path)\n",
    "            print(f\"Loaded foul_drawing: {len(data_dict['fouls'])} players\")\n",
    "            \n",
    "        return data_dict\n",
    "    \n",
    "    def load_team_data(self, team_name):\n",
    "        \"\"\"Load team-level data for pace and efficiency\"\"\"\n",
    "        team_path = self.team_data_path / team_name.lower().replace(' ', '_')\n",
    "        \n",
    "        if not team_path.exists():\n",
    "            print(f\"Team path not found: {team_path}\")\n",
    "            return None\n",
    "            \n",
    "        team_data = {}\n",
    "        \n",
    "        # Load team efficiency\n",
    "        efficiency_path = team_path / 'team_efficiency_and_four_factors_all_seasons.csv'\n",
    "        if efficiency_path.exists():\n",
    "            team_data['efficiency'] = pd.read_csv(efficiency_path)\n",
    "            \n",
    "        return team_data\n",
    "\n",
    "# Initialize data loader\n",
    "loader = CTGDataLoader()\n",
    "\n",
    "# Load 2023-24 regular season data\n",
    "season_data = loader.load_player_season_data('2023-24', 'regular_season')\n",
    "\n",
    "if season_data:\n",
    "    print(f\"\\nSuccessfully loaded {len(season_data)} data categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_percentage_columns(df, percentage_cols):\n",
    "    \"\"\"Clean percentage columns by removing % sign and converting to float\"\"\"\n",
    "    for col in percentage_cols:\n",
    "        if col in df.columns:\n",
    "            # Handle percentage strings\n",
    "            if df[col].dtype == 'object':\n",
    "                df[col] = df[col].str.replace('%', '').astype(float) / 100\n",
    "            # Fill NaN with 0 for percentage columns\n",
    "            df[col] = df[col].fillna(0)\n",
    "    return df\n",
    "\n",
    "def merge_player_data(data_dict):\n",
    "    \"\"\"Merge all player data sources into single dataframe\"\"\"\n",
    "    \n",
    "    # Start with offensive data as base\n",
    "    if 'offensive' not in data_dict:\n",
    "        print(\"No offensive data found\")\n",
    "        return None\n",
    "        \n",
    "    merged_df = data_dict['offensive'].copy()\n",
    "    \n",
    "    # Merge defense/rebounding data\n",
    "    if 'defense' in data_dict:\n",
    "        defense_cols = ['Player', 'Team', 'MIN', 'fgOR%', 'fgDR%', 'ftOR%', 'ftDR%', \n",
    "                       'BLK%', 'STL%', 'FOUL%']\n",
    "        defense_cols = [col for col in defense_cols if col in data_dict['defense'].columns]\n",
    "        merged_df = merged_df.merge(\n",
    "            data_dict['defense'][defense_cols],\n",
    "            on=['Player', 'Team'],\n",
    "            how='left',\n",
    "            suffixes=('', '_defense')\n",
    "        )\n",
    "    \n",
    "    # Merge shooting data\n",
    "    if 'shooting' in data_dict:\n",
    "        shooting_cols = ['Player', 'Team', 'eFG%', '2P%', '3P%', 'FT%']\n",
    "        shooting_cols = [col for col in shooting_cols if col in data_dict['shooting'].columns]\n",
    "        merged_df = merged_df.merge(\n",
    "            data_dict['shooting'][shooting_cols],\n",
    "            on=['Player', 'Team'],\n",
    "            how='left',\n",
    "            suffixes=('', '_shooting')\n",
    "        )\n",
    "        \n",
    "    # Merge shooting accuracy (zone-specific)\n",
    "    if 'shooting_accuracy' in data_dict:\n",
    "        accuracy_cols = ['Player', 'Team', 'Rim FG%', 'Short Mid FG%', 'Long Mid FG%',\n",
    "                        'Corner Three FG%', 'Non-Corner Three FG%']\n",
    "        accuracy_cols = [col for col in accuracy_cols if col in data_dict['shooting_accuracy'].columns]\n",
    "        merged_df = merged_df.merge(\n",
    "            data_dict['shooting_accuracy'][accuracy_cols],\n",
    "            on=['Player', 'Team'],\n",
    "            how='left',\n",
    "            suffixes=('', '_accuracy')\n",
    "        )\n",
    "    \n",
    "    # Merge foul drawing\n",
    "    if 'fouls' in data_dict:\n",
    "        foul_cols = ['Player', 'Team', 'SFLD%', 'FFLD%', 'AND1%']\n",
    "        foul_cols = [col for col in foul_cols if col in data_dict['fouls'].columns]\n",
    "        merged_df = merged_df.merge(\n",
    "            data_dict['fouls'][foul_cols],\n",
    "            on=['Player', 'Team'],\n",
    "            how='left',\n",
    "            suffixes=('', '_fouls')\n",
    "        )\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Merge all player data\n",
    "if season_data:\n",
    "    player_df = merge_player_data(season_data)\n",
    "    print(f\"Merged dataframe shape: {player_df.shape}\")\n",
    "    print(f\"\\nColumns available: {list(player_df.columns)[:20]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tier 1: Core Performance Engine Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorePerformanceFeatures:\n",
    "    \"\"\"Generate Tier 1 features - Player's baseline abilities\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_features(df):\n",
    "        \"\"\"Create core performance features\"\"\"\n",
    "        features = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        # 1. Usage Rate (already in data as 'Usage')\n",
    "        if 'Usage' in df.columns:\n",
    "            features['USG_percent'] = df['Usage']\n",
    "        \n",
    "        # 2. Points Per Shot Attempt (PSA) - already in data\n",
    "        if 'PSA' in df.columns:\n",
    "            features['PSA'] = df['PSA']\n",
    "            \n",
    "        # 3. AST to Usage Ratio\n",
    "        if 'AST%' in df.columns and 'Usage' in df.columns:\n",
    "            features['AST_to_USG_Ratio'] = df['AST%'] / (df['Usage'] + 0.001)  # Avoid division by zero\n",
    "            \n",
    "        # 4. Rebounding percentages\n",
    "        if 'fgDR%' in df.columns:\n",
    "            features['fgDR_percent'] = df['fgDR%']\n",
    "        if 'fgOR%' in df.columns:\n",
    "            features['fgOR_percent'] = df['fgOR%']\n",
    "            \n",
    "        # 5. Total Rebounding Rate\n",
    "        if 'fgDR%' in df.columns and 'fgOR%' in df.columns:\n",
    "            features['Total_REB_percent'] = df['fgDR%'] + df['fgOR%']\n",
    "            \n",
    "        # 6. Shooting Efficiency\n",
    "        if 'eFG%' in df.columns:\n",
    "            features['eFG_percent'] = df['eFG%']\n",
    "            \n",
    "        # 7. Turnover Rate\n",
    "        if 'TOV%' in df.columns:\n",
    "            features['TOV_percent'] = df['TOV%']\n",
    "            \n",
    "        # 8. Defensive Impact\n",
    "        if 'BLK%' in df.columns and 'STL%' in df.columns:\n",
    "            features['Defensive_Activity'] = df['BLK%'] + df['STL%']\n",
    "            \n",
    "        # 9. Foul Drawing Ability\n",
    "        if 'SFLD%' in df.columns:\n",
    "            features['Foul_Drawing'] = df['SFLD%']\n",
    "            \n",
    "        # 10. Create PER approximation\n",
    "        if all(col in df.columns for col in ['Usage', 'PSA', 'AST%', 'fgDR%', 'BLK%', 'STL%', 'TOV%']):\n",
    "            features['PER_approx'] = (\n",
    "                df['Usage'] * 0.3 +\n",
    "                df['PSA'] * 0.2 +\n",
    "                df['AST%'] * 0.15 +\n",
    "                df['fgDR%'] * 0.15 +\n",
    "                (df['BLK%'] + df['STL%']) * 0.1 -\n",
    "                df['TOV%'] * 0.1\n",
    "            )\n",
    "            \n",
    "        return features\n",
    "\n",
    "# Generate Core Performance features\n",
    "if 'player_df' in locals():\n",
    "    core_features = CorePerformanceFeatures.create_features(player_df)\n",
    "    print(f\"Generated {len(core_features.columns)} core performance features:\")\n",
    "    print(core_features.columns.tolist())\n",
    "    print(f\"\\nSample data:\")\n",
    "    print(core_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tier 2: Contextual Modulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualModulators:\n",
    "    \"\"\"Generate Tier 2 features - Game-specific factors\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_features(df):\n",
    "        \"\"\"Create contextual features\"\"\"\n",
    "        features = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        # 1. Minutes Played (season average as proxy for expected minutes)\n",
    "        if 'MIN' in df.columns:\n",
    "            features['Minutes_Season_Avg'] = df['MIN']\n",
    "            \n",
    "            # Categorize minutes into bins\n",
    "            features['Minutes_Category'] = pd.cut(\n",
    "                df['MIN'], \n",
    "                bins=[0, 15, 25, 32, 40],\n",
    "                labels=['Bench', 'Rotation', 'Starter', 'Star']\n",
    "            )\n",
    "            \n",
    "        # 2. Position (if available) - for now we'll infer from stats\n",
    "        # High AST% = Guard, High REB% = Big, Balanced = Forward\n",
    "        if 'AST%' in df.columns and 'fgDR%' in df.columns:\n",
    "            conditions = [\n",
    "                (df['AST%'] > df['AST%'].quantile(0.7)),  # Guards\n",
    "                (df['fgDR%'] > df['fgDR%'].quantile(0.7)),  # Centers\n",
    "            ]\n",
    "            choices = ['Guard', 'Big']\n",
    "            features['Position_Inferred'] = np.select(conditions, choices, default='Forward')\n",
    "            \n",
    "        # 3. Role Classification based on usage and minutes\n",
    "        if 'Usage' in df.columns and 'MIN' in df.columns:\n",
    "            conditions = [\n",
    "                (df['Usage'] > 25) & (df['MIN'] > 30),  # Primary Option\n",
    "                (df['Usage'] > 20) & (df['MIN'] > 25),  # Secondary Option\n",
    "                (df['MIN'] > 20),  # Role Player\n",
    "            ]\n",
    "            choices = ['Primary', 'Secondary', 'Role']\n",
    "            features['Player_Role'] = np.select(conditions, choices, default='Bench')\n",
    "            \n",
    "        # 4. Efficiency Context\n",
    "        if 'PSA' in df.columns:\n",
    "            features['Efficiency_Level'] = pd.cut(\n",
    "                df['PSA'],\n",
    "                bins=df['PSA'].quantile([0, 0.25, 0.5, 0.75, 1.0]),\n",
    "                labels=['Low', 'Below_Avg', 'Above_Avg', 'Elite'],\n",
    "                duplicates='drop'\n",
    "            )\n",
    "            \n",
    "        return features\n",
    "\n",
    "# Generate Contextual features\n",
    "if 'player_df' in locals():\n",
    "    context_features = ContextualModulators.create_features(player_df)\n",
    "    print(f\"Generated {len(context_features.columns)} contextual features:\")\n",
    "    print(context_features.columns.tolist())\n",
    "    print(f\"\\nSample data:\")\n",
    "    print(context_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tier 3: Temporal Dynamics (Simulated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalDynamics:\n",
    "    \"\"\"Generate Tier 3 features - Recent form and trends\n",
    "    Note: These would normally use game-by-game data. \n",
    "    For now, we'll create simulated versions based on season averages.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_features(df):\n",
    "        \"\"\"Create temporal features (simulated for demonstration)\"\"\"\n",
    "        features = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        # Since we don't have game-by-game data yet, we'll create proxy features\n",
    "        # In production, these would be calculated from actual game logs\n",
    "        \n",
    "        # 1. Consistency Score (based on ranking stability)\n",
    "        if 'Usage Rank' in df.columns and 'PSA Rank' in df.columns:\n",
    "            # Lower rank variance = more consistent\n",
    "            features['Consistency_Score'] = 1 / (1 + np.abs(df['Usage Rank'] - df['PSA Rank'])/100)\n",
    "            \n",
    "        # 2. Volatility Proxy (based on position in distributions)\n",
    "        if 'Usage' in df.columns:\n",
    "            # Players at extremes tend to be more consistent\n",
    "            usage_zscore = np.abs(stats.zscore(df['Usage'].fillna(df['Usage'].mean())))\n",
    "            features['Usage_Stability'] = 1 / (1 + np.exp(-usage_zscore))\n",
    "            \n",
    "        # 3. Performance Tier (for trend proxy)\n",
    "        if 'MIN' in df.columns and 'Usage' in df.columns:\n",
    "            # Higher minutes and usage = likely trending up\n",
    "            features['Performance_Tier'] = (\n",
    "                df['MIN'].rank(pct=True) * 0.5 + \n",
    "                df['Usage'].rank(pct=True) * 0.5\n",
    "            )\n",
    "            \n",
    "        # 4. Opportunity Score\n",
    "        if all(col in df.columns for col in ['MIN', 'Usage', 'PSA']):\n",
    "            features['Opportunity_Score'] = (\n",
    "                df['MIN'] * df['Usage'] * df['PSA'] / 10000\n",
    "            )\n",
    "            \n",
    "        return features\n",
    "\n",
    "# Generate Temporal features\n",
    "if 'player_df' in locals():\n",
    "    temporal_features = TemporalDynamics.create_features(player_df)\n",
    "    print(f\"Generated {len(temporal_features.columns)} temporal features:\")\n",
    "    print(temporal_features.columns.tolist())\n",
    "    print(f\"\\nSample data:\")\n",
    "    print(temporal_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Combine All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_all_features(player_df, core_features, context_features, temporal_features):\n",
    "    \"\"\"Combine all feature tiers into single dataframe\"\"\"\n",
    "    \n",
    "    # Start with player identifiers\n",
    "    combined_df = player_df[['Player', 'Team']].copy()\n",
    "    \n",
    "    # Add target variable proxies (would be actual PRA in production)\n",
    "    if all(col in player_df.columns for col in ['MIN', 'Usage', 'PSA', 'AST%', 'fgDR%']):\n",
    "        # Estimate PRA based on available stats\n",
    "        combined_df['PRA_estimate'] = (\n",
    "            player_df['MIN'] * player_df['Usage'] * player_df['PSA'] / 500 +  # Points proxy\n",
    "            player_df['MIN'] * player_df['fgDR%'] * 10 +  # Rebounds proxy\n",
    "            player_df['MIN'] * player_df['AST%'] * 5  # Assists proxy\n",
    "        )\n",
    "    \n",
    "    # Combine all features\n",
    "    combined_df = pd.concat([\n",
    "        combined_df,\n",
    "        core_features,\n",
    "        context_features,\n",
    "        temporal_features\n",
    "    ], axis=1)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Combine all features\n",
    "if all(var in locals() for var in ['player_df', 'core_features', 'context_features', 'temporal_features']):\n",
    "    final_features = combine_all_features(player_df, core_features, context_features, temporal_features)\n",
    "    \n",
    "    print(f\"Final feature matrix shape: {final_features.shape}\")\n",
    "    print(f\"\\nFeature categories:\")\n",
    "    print(f\"  - Core Performance: {len(core_features.columns)} features\")\n",
    "    print(f\"  - Contextual: {len(context_features.columns)} features\")\n",
    "    print(f\"  - Temporal: {len(temporal_features.columns)} features\")\n",
    "    print(f\"  - Total: {len(final_features.columns)} columns\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(f\"\\nTop 5 players by PRA estimate:\")\n",
    "    print(final_features.nlargest(5, 'PRA_estimate')[['Player', 'Team', 'PRA_estimate']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Analysis & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numeric features only\n",
    "if 'final_features' in locals():\n",
    "    numeric_features = final_features.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_features = [f for f in numeric_features if f != 'PRA_estimate']  # Exclude target\n",
    "    \n",
    "    print(f\"Numeric features for analysis: {len(numeric_features)}\")\n",
    "    print(numeric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distributions\n",
    "if 'final_features' in locals() and len(numeric_features) > 0:\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(15, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, feature in enumerate(numeric_features[:12]):\n",
    "        final_features[feature].hist(bins=30, ax=axes[i], edgecolor='black')\n",
    "        axes[i].set_title(feature)\n",
    "        axes[i].set_xlabel('Value')\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Feature Distributions', y=1.02, fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlations with target\n",
    "if 'final_features' in locals() and 'PRA_estimate' in final_features.columns:\n",
    "    correlations = final_features[numeric_features].corrwith(final_features['PRA_estimate']).sort_values(ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    correlations.head(15).plot(kind='barh')\n",
    "    plt.xlabel('Correlation with PRA Estimate')\n",
    "    plt.title('Top 15 Features by Correlation with Target')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTop correlations with PRA:\")\n",
    "    print(correlations.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance using mutual information\n",
    "if 'final_features' in locals() and len(numeric_features) > 0:\n",
    "    from sklearn.feature_selection import mutual_info_regression\n",
    "    \n",
    "    # Prepare data\n",
    "    X = final_features[numeric_features].fillna(0)\n",
    "    y = final_features['PRA_estimate'].fillna(0)\n",
    "    \n",
    "    # Calculate mutual information\n",
    "    mi_scores = mutual_info_regression(X, y, random_state=42)\n",
    "    mi_scores = pd.Series(mi_scores, index=numeric_features).sort_values(ascending=False)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    mi_scores.head(15).plot(kind='barh')\n",
    "    plt.xlabel('Mutual Information Score')\n",
    "    plt.title('Top 15 Features by Mutual Information')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTop features by mutual information:\")\n",
    "    print(mi_scores.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_quality_report(df, numeric_features):\n",
    "    \"\"\"Generate comprehensive feature quality report\"\"\"\n",
    "    \n",
    "    report = pd.DataFrame(index=numeric_features)\n",
    "    \n",
    "    # Basic statistics\n",
    "    report['mean'] = df[numeric_features].mean()\n",
    "    report['std'] = df[numeric_features].std()\n",
    "    report['min'] = df[numeric_features].min()\n",
    "    report['max'] = df[numeric_features].max()\n",
    "    \n",
    "    # Missing values\n",
    "    report['missing_count'] = df[numeric_features].isna().sum()\n",
    "    report['missing_pct'] = (report['missing_count'] / len(df)) * 100\n",
    "    \n",
    "    # Zeros\n",
    "    report['zero_count'] = (df[numeric_features] == 0).sum()\n",
    "    report['zero_pct'] = (report['zero_count'] / len(df)) * 100\n",
    "    \n",
    "    # Outliers (using IQR method)\n",
    "    Q1 = df[numeric_features].quantile(0.25)\n",
    "    Q3 = df[numeric_features].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = ((df[numeric_features] < (Q1 - 1.5 * IQR)) | (df[numeric_features] > (Q3 + 1.5 * IQR))).sum()\n",
    "    report['outlier_count'] = outliers\n",
    "    report['outlier_pct'] = (outliers / len(df)) * 100\n",
    "    \n",
    "    # Variance\n",
    "    report['variance'] = df[numeric_features].var()\n",
    "    \n",
    "    # Skewness\n",
    "    report['skewness'] = df[numeric_features].skew()\n",
    "    \n",
    "    return report.round(3)\n",
    "\n",
    "# Generate quality report\n",
    "if 'final_features' in locals() and len(numeric_features) > 0:\n",
    "    quality_report = generate_feature_quality_report(final_features, numeric_features)\n",
    "    \n",
    "    print(\"Feature Quality Report:\")\n",
    "    print(\"=\"*50)\n",
    "    print(quality_report.head(10))\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nQuality Summary:\")\n",
    "    print(f\"Features with >10% missing: {(quality_report['missing_pct'] > 10).sum()}\")\n",
    "    print(f\"Features with >50% zeros: {(quality_report['zero_pct'] > 50).sum()}\")\n",
    "    print(f\"Features with >10% outliers: {(quality_report['outlier_pct'] > 10).sum()}\")\n",
    "    print(f\"Features with low variance (<0.01): {(quality_report['variance'] < 0.01).sum()}\")\n",
    "    print(f\"Highly skewed features (|skew| > 2): {(np.abs(quality_report['skewness']) > 2).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Processed Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the feature matrix\n",
    "if 'final_features' in locals():\n",
    "    output_path = Path('/Users/diyagamah/Documents/nba_props_model/data/processed')\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save features\n",
    "    final_features.to_csv(output_path / 'player_features_2023_24.csv', index=False)\n",
    "    print(f\"Saved features to: {output_path / 'player_features_2023_24.csv'}\")\n",
    "    \n",
    "    # Save quality report\n",
    "    quality_report.to_csv(output_path / 'feature_quality_report.csv')\n",
    "    print(f\"Saved quality report to: {output_path / 'feature_quality_report.csv'}\")\n",
    "    \n",
    "    # Save feature list\n",
    "    with open(output_path / 'feature_list.txt', 'w') as f:\n",
    "        f.write(\"Core Performance Features:\\n\")\n",
    "        for feat in core_features.columns:\n",
    "            f.write(f\"  - {feat}\\n\")\n",
    "        f.write(\"\\nContextual Modulator Features:\\n\")\n",
    "        for feat in context_features.columns:\n",
    "            f.write(f\"  - {feat}\\n\")\n",
    "        f.write(\"\\nTemporal Dynamic Features:\\n\")\n",
    "        for feat in temporal_features.columns:\n",
    "            f.write(f\"  - {feat}\\n\")\n",
    "    print(f\"Saved feature list to: {output_path / 'feature_list.txt'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps\n",
    "\n",
    "### Completed ✅\n",
    "1. Loaded CTG player data for 2023-24 season\n",
    "2. Created three-tier feature architecture:\n",
    "   - **Tier 1**: Core Performance (10 features)\n",
    "   - **Tier 2**: Contextual Modulators (5 features)\n",
    "   - **Tier 3**: Temporal Dynamics (4 features)\n",
    "3. Generated feature quality report\n",
    "4. Analyzed feature correlations and importance\n",
    "\n",
    "### To Do 📋\n",
    "1. **Load game-by-game data** to calculate real temporal features (rolling averages, EWMA)\n",
    "2. **Add opponent data** for matchup-specific features\n",
    "3. **Integrate team pace data** for context features\n",
    "4. **Add injury/lineup data** for On/Off usage deltas\n",
    "5. **Create training pipeline** with proper train/test splits\n",
    "6. **Build prediction models** (start with baseline, then advanced)\n",
    "7. **Implement backtesting** on historical data\n",
    "\n",
    "### Key Insights 🔍\n",
    "- Usage Rate and Minutes are strongest predictors (expected)\n",
    "- Efficiency metrics (PSA, eFG%) show strong correlation\n",
    "- Rebounding percentages provide good signal for big men\n",
    "- Need game-level data for proper temporal features\n",
    "- Position inference from stats works reasonably well"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nba-props-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
