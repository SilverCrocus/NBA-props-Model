{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA Props Model - Fixed Evaluation\n",
    "\n",
    "## Critical Issues Addressed:\n",
    "1. **Removed data leakage features** (Opportunity_Score, etc.)\n",
    "2. **Simplified models** for 503 samples\n",
    "3. **Fixed preprocessing** pipeline\n",
    "4. **Realistic expectations** set\n",
    "\n",
    "### ⚠️ IMPORTANT:\n",
    "The current PRA_estimate is calculated from features, not real game data. \n",
    "This notebook shows what to expect with REAL PRA data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Cleaned Data (Without Leakage Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned dataset\n",
    "data_path = Path('/Users/diyagamah/Documents/nba_props_model/data/processed')\n",
    "df = pd.read_csv(data_path / 'player_features_2023_24_clean.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Players: {len(df)}\")\n",
    "\n",
    "# Identify features and target\n",
    "feature_cols = [col for col in df.columns if col not in ['Player', 'Team', 'PRA_estimate']]\n",
    "X = df[feature_cols].copy()\n",
    "y = df['PRA_estimate'].copy()\n",
    "\n",
    "print(f\"\\nFeatures ({len(feature_cols)}):\")\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(f\"  Mean: {y.mean():.2f}\")\n",
    "print(f\"  Std: {y.std():.2f}\")\n",
    "print(f\"  Range: [{y.min():.2f}, {y.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Three-Way Split (Train/Validation/Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split: separate test set\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: separate train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Data Split:\")\n",
    "print(f\"  Training: {len(X_train)} samples\")\n",
    "print(f\"  Validation: {len(X_val)} samples\")\n",
    "print(f\"  Test: {len(X_test)} samples (NEVER touched during training)\")\n",
    "\n",
    "# Scale features\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nFeatures scaled using RobustScaler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simple Models (Appropriate for 503 Samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SIMPLE models appropriate for small dataset\n",
    "models = {\n",
    "    'Ridge (α=10)': Ridge(alpha=10.0, random_state=42),\n",
    "    'Ridge (α=1)': Ridge(alpha=1.0, random_state=42),\n",
    "    'Lasso (α=1)': Lasso(alpha=1.0, random_state=42),\n",
    "    'ElasticNet': ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42),\n",
    "    'XGBoost (Simple)': xgb.XGBRegressor(\n",
    "        n_estimators=50,  # Reduced from 200\n",
    "        max_depth=3,      # Reduced from 6\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.7,\n",
    "        reg_alpha=1.0,    # L1 regularization\n",
    "        reg_lambda=1.0,   # L2 regularization\n",
    "        random_state=42\n",
    "    ),\n",
    "    'RandomForest (Simple)': RandomForestRegressor(\n",
    "        n_estimators=50,  # Reduced\n",
    "        max_depth=5,      # Limited depth\n",
    "        min_samples_split=10,  # Prevent overfitting\n",
    "        min_samples_leaf=5,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "print(f\"Training {len(models)} simple models...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate on VALIDATION set\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Use scaled data for linear models, raw for tree models\n",
    "    if 'XGB' in name or 'Forest' in name:\n",
    "        model.fit(X_train, y_train)\n",
    "        val_pred = model.predict(X_val)\n",
    "    else:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        val_pred = model.predict(X_val_scaled)\n",
    "    \n",
    "    # Calculate metrics on validation set\n",
    "    val_mae = mean_absolute_error(y_val, val_pred)\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "    val_r2 = r2_score(y_val, val_pred)\n",
    "    val_mape = np.mean(np.abs((y_val - val_pred) / y_val)) * 100\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'val_mae': val_mae,\n",
    "        'val_rmse': val_rmse,\n",
    "        'val_r2': val_r2,\n",
    "        'val_mape': val_mape\n",
    "    }\n",
    "    \n",
    "    print(f\"  Validation MAE: {val_mae:.3f}\")\n",
    "    print(f\"  Validation R²: {val_r2:.3f}\")\n",
    "    print(f\"  Validation MAPE: {val_mape:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-Validation (More Reliable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold CV on train+val data\n",
    "X_train_val = np.vstack([X_train, X_val])\n",
    "y_train_val = np.concatenate([y_train, y_val])\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"5-Fold Cross-Validation Results:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "cv_results = {}\n",
    "for name, model_info in results.items():\n",
    "    model = models[name]  # Fresh model\n",
    "    \n",
    "    if 'XGB' in name or 'Forest' in name:\n",
    "        data_to_use = np.vstack([X_train, X_val])\n",
    "    else:\n",
    "        data_to_use = np.vstack([X_train_scaled, X_val_scaled])\n",
    "    \n",
    "    cv_scores = cross_val_score(\n",
    "        model, data_to_use, y_train_val,\n",
    "        cv=kfold, scoring='neg_mean_absolute_error'\n",
    "    )\n",
    "    cv_scores = -cv_scores\n",
    "    \n",
    "    cv_results[name] = {\n",
    "        'mean': cv_scores.mean(),\n",
    "        'std': cv_scores.std()\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Mean MAE: {cv_scores.mean():.3f} (±{cv_scores.std():.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Select Best Model and Final Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on validation performance\n",
    "best_model_name = min(results, key=lambda x: results[x]['val_mae'])\n",
    "best_model = results[best_model_name]['model']\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Validation MAE: {results[best_model_name]['val_mae']:.3f}\")\n",
    "\n",
    "# FINAL TEST on held-out test set\n",
    "if 'XGB' in best_model_name or 'Forest' in best_model_name:\n",
    "    test_pred = best_model.predict(X_test)\n",
    "else:\n",
    "    test_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "test_mae = mean_absolute_error(y_test, test_pred)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "test_r2 = r2_score(y_test, test_pred)\n",
    "test_mape = np.mean(np.abs((y_test - test_pred) / y_test)) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL TEST RESULTS (Never seen during training):\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Test MAE: {test_mae:.3f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "print(f\"Test R²: {test_r2:.3f}\")\n",
    "print(f\"Test MAPE: {test_mape:.1f}%\")\n",
    "\n",
    "# Check for overfitting\n",
    "val_performance = results[best_model_name]['val_mae']\n",
    "overfit_ratio = test_mae / val_performance\n",
    "print(f\"\\nOverfitting Check:\")\n",
    "print(f\"  Val MAE: {val_performance:.3f}\")\n",
    "print(f\"  Test MAE: {test_mae:.3f}\")\n",
    "print(f\"  Ratio: {overfit_ratio:.2f} (close to 1.0 is good)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# 1. Actual vs Predicted\n",
    "ax1 = axes[0, 0]\n",
    "ax1.scatter(y_test, test_pred, alpha=0.5)\n",
    "ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "ax1.set_xlabel('Actual PRA')\n",
    "ax1.set_ylabel('Predicted PRA')\n",
    "ax1.set_title(f'Test Predictions ({best_model_name})')\n",
    "ax1.text(0.05, 0.95, f'R² = {test_r2:.3f}\\nMAE = {test_mae:.3f}',\n",
    "         transform=ax1.transAxes, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# 2. Residuals\n",
    "ax2 = axes[0, 1]\n",
    "residuals = y_test - test_pred\n",
    "ax2.scatter(test_pred, residuals, alpha=0.5)\n",
    "ax2.axhline(y=0, color='r', linestyle='--')\n",
    "ax2.set_xlabel('Predicted PRA')\n",
    "ax2.set_ylabel('Residuals')\n",
    "ax2.set_title('Residual Plot')\n",
    "ax2.text(0.05, 0.95, f'Mean: {residuals.mean():.2f}\\nStd: {residuals.std():.2f}',\n",
    "         transform=ax2.transAxes, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# 3. Error Distribution\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(residuals, bins=20, edgecolor='black', alpha=0.7)\n",
    "ax3.axvline(x=0, color='r', linestyle='--')\n",
    "ax3.set_xlabel('Prediction Error')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.set_title('Error Distribution')\n",
    "\n",
    "# 4. Model Comparison\n",
    "ax4 = axes[1, 1]\n",
    "model_names = list(results.keys())\n",
    "val_maes = [results[m]['val_mae'] for m in model_names]\n",
    "colors = ['green' if m == best_model_name else 'steelblue' for m in model_names]\n",
    "bars = ax4.bar(range(len(model_names)), val_maes, color=colors)\n",
    "ax4.set_xticks(range(len(model_names)))\n",
    "ax4.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "ax4.set_ylabel('Validation MAE')\n",
    "ax4.set_title('Model Comparison')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Fixed Model Evaluation Results', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. What These Results Mean\n",
    "\n",
    "### Current Performance (with calculated PRA_estimate):\n",
    "- The high R² values you're seeing are **fake** - the model is learning a formula\n",
    "- This is NOT predictive performance\n",
    "\n",
    "### Expected Performance (with REAL PRA data):\n",
    "- **R² = 0.35-0.50**: This is realistic for NBA predictions\n",
    "- **MAE = 3-5 points**: Real prediction error for PRA\n",
    "- **MAPE = 25-35%**: Typical percentage error\n",
    "\n",
    "### Next Steps:\n",
    "1. **Get real PRA data** from NBA games\n",
    "2. **Add temporal features**: Recent game performance\n",
    "3. **Include context**: Opponent, home/away, rest days\n",
    "4. **Collect more data**: Need 2000+ player-games minimum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}