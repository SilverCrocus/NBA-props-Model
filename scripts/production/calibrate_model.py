#!/usr/bin/env python3
"""
Model Calibration Script - Isotonic Regression

Calibrates XGBoost PRA predictions for betting using isotonic regression.
Research shows calibration is MORE important than accuracy for betting profitability.

Walsh & Joshi (2023):
- Calibration-optimized model: +34.69% ROI
- Accuracy-optimized model: -35.17% ROI (lost money!)

Usage:
    uv run python scripts/production/calibrate_model.py

Inputs:
    - models/production_model_v2.0_CLEAN_latest.pkl
    - data/historical_odds/2024-25/pra_odds.csv

Outputs:
    - models/production_model_v2.0_CLEAN_CALIBRATED_latest.pkl
    - data/results/calibration_evaluation.csv
"""

import pandas as pd
import numpy as np
import pickle
from pathlib import Path
from sklearn.isotonic import IsotonicRegression
from sklearn.metrics import brier_score_loss, log_loss
import matplotlib.pyplot as plt
import sys

sys.path.append(str(Path(__file__).parent.parent.parent))

print("="*80)
print("MODEL CALIBRATION - Isotonic Regression")
print("="*80)

# ============================================================================
# 1. LOAD MODEL
# ============================================================================

print("\n1. Loading model...")
model_path = 'models/production_model_v2.0_CLEAN_latest.pkl'

with open(model_path, 'rb') as f:
    model_dict = pickle.load(f)

model = model_dict['model']
feature_cols = model_dict['feature_cols']

print(f"✅ Loaded model: {model_dict['version']}")
print(f"   Train MAE: {model_dict['train_mae']:.2f}")
print(f"   Val MAE: {model_dict['val_mae']:.2f}")
print(f"   Features: {len(feature_cols)}")

# ============================================================================
# 2. LOAD VALIDATION PREDICTIONS AND ODDS
# ============================================================================

print("\n2. Loading 2023-24 validation predictions and betting odds...")

# Load validation predictions (generated by generate_val_predictions_2023_24.py)
val_predictions_path = 'data/processed/val_predictions_2023_24.csv'
val_df = pd.read_csv(val_predictions_path)
val_df['GAME_DATE'] = pd.to_datetime(val_df['GAME_DATE'])

print(f"✅ Loaded {len(val_df):,} validation predictions")
print(f"   Date range: {val_df['GAME_DATE'].min().date()} to {val_df['GAME_DATE'].max().date()}")
print(f"   MAE: {val_df['error'].mean():.2f} points")

# Load historical odds for validation period
odds_df = pd.read_csv('data/historical_odds/2023-24/pra_odds.csv')
odds_df['event_date'] = pd.to_datetime(odds_df['event_date'])

print(f"✅ Loaded {len(odds_df):,} odds lines")

# Merge validation predictions with odds
merged_df = val_df.merge(
    odds_df,
    left_on=['PLAYER_NAME', 'GAME_DATE'],
    right_on=['player_name', 'event_date'],
    how='inner'
)

print(f"✅ Matched {len(merged_df):,} predictions with odds")

if len(merged_df) == 0:
    print("\n❌ No validation games have odds data!")
    print("   Cannot calibrate without odds. Options:")
    print("   1. Use 2024-25 data instead (but this is the test set)")
    print("   2. Scrape 2023-24 historical odds")
    print("   3. Skip calibration for now (not recommended)")
    sys.exit(1)

# ============================================================================
# 3. PREPARE DATA FOR CALIBRATION
# ============================================================================

print("\n3. Preparing data for calibration...")

# We already have predictions from Step 2
val_predictions = merged_df['predicted_PRA'].values
y_val = merged_df['actual_PRA'].values

print(f"✅ Prepared {len(val_predictions):,} predictions")
print(f"   Prediction range: {val_predictions.min():.1f} to {val_predictions.max():.1f}")
print(f"   Actual range: {y_val.min():.1f} to {y_val.max():.1f}")

# ============================================================================
# 4. CONVERT TO WIN PROBABILITIES
# ============================================================================

print("\n4. Converting predictions to win probabilities...")

def american_to_decimal(american_odds):
    """Convert American odds to decimal odds"""
    if american_odds > 0:
        return 1 + (american_odds / 100)
    else:
        return 1 + (100 / abs(american_odds))

# For each game, convert predicted PRA vs line into P(over)
calibration_data = []

for i, row in merged_df.iterrows():
    pred_pra = row['predicted_PRA']
    actual_pra = row['actual_PRA']
    line = row['line']

    # Logistic transform: distance from line → probability
    # If we predict 30 and line is 25, we think P(over) is high
    # If we predict 20 and line is 25, we think P(over) is low
    distance_from_line = pred_pra - line

    # Simple logistic: P(over) = 1 / (1 + exp(-distance / scale))
    # Scale = typical MAE (controls steepness)
    scale = 5.0  # Approximate MAE
    prob_over_raw = 1 / (1 + np.exp(-distance_from_line / scale))

    # Ground truth
    actual_over = 1 if actual_pra > line else 0

    calibration_data.append({
        'player': row['PLAYER_NAME'],
        'game_date': row['GAME_DATE'],
        'predicted_pra': pred_pra,
        'actual_pra': actual_pra,
        'line': line,
        'prob_over_raw': prob_over_raw,
        'actual_over': actual_over
    })

calib_df = pd.DataFrame(calibration_data)

print(f"✅ Converted {len(calib_df):,} predictions to probabilities")
print(f"   Raw probability range: {calib_df['prob_over_raw'].min():.3f} to {calib_df['prob_over_raw'].max():.3f}")
print(f"   Actual over rate: {calib_df['actual_over'].mean():.1%}")

# ============================================================================
# 5. TRAIN ISOTONIC CALIBRATOR
# ============================================================================

print("\n5. Training isotonic regression calibrator...")

# Isotonic regression learns monotonic mapping: raw_prob → calibrated_prob
calibrator = IsotonicRegression(out_of_bounds='clip')

X_calib = calib_df['prob_over_raw'].values
y_calib = calib_df['actual_over'].values

calibrator.fit(X_calib, y_calib)

# Get calibrated probabilities
calib_df['prob_over_calibrated'] = calibrator.predict(calib_df['prob_over_raw'])

print(f"✅ Calibrator trained")
print(f"   Calibrated probability range: {calib_df['prob_over_calibrated'].min():.3f} to {calib_df['prob_over_calibrated'].max():.3f}")

# ============================================================================
# 6. EVALUATE CALIBRATION QUALITY
# ============================================================================

print("\n6. Evaluating calibration quality...")

# Brier score (lower is better, target: <0.15)
brier_raw = brier_score_loss(y_calib, calib_df['prob_over_raw'])
brier_calibrated = brier_score_loss(y_calib, calib_df['prob_over_calibrated'])

# Log loss (lower is better)
log_loss_raw = log_loss(y_calib, calib_df['prob_over_raw'])
log_loss_calibrated = log_loss(y_calib, calib_df['prob_over_calibrated'])

print(f"\nCalibration Metrics:")
print(f"  Raw model:")
print(f"    Brier score: {brier_raw:.4f}")
print(f"    Log loss: {log_loss_raw:.4f}")
print(f"  Calibrated model:")
print(f"    Brier score: {brier_calibrated:.4f} ({'✅ BETTER' if brier_calibrated < brier_raw else '❌ WORSE'})")
print(f"    Log loss: {log_loss_calibrated:.4f} ({'✅ BETTER' if log_loss_calibrated < log_loss_raw else '❌ WORSE'})")

if brier_calibrated < 0.15:
    print(f"\n✅ Brier score < 0.15 (EXCELLENT calibration)")
elif brier_calibrated < 0.20:
    print(f"\n⚠️  Brier score 0.15-0.20 (GOOD calibration)")
else:
    print(f"\n❌ Brier score > 0.20 (POOR calibration)")

# Save evaluation results
eval_df = pd.DataFrame({
    'metric': ['brier_score', 'log_loss'],
    'raw': [brier_raw, log_loss_raw],
    'calibrated': [brier_calibrated, log_loss_calibrated],
    'improvement': [brier_raw - brier_calibrated, log_loss_raw - log_loss_calibrated]
})

eval_path = 'data/results/calibration_evaluation.csv'
eval_df.to_csv(eval_path, index=False)
print(f"\n✅ Evaluation saved to {eval_path}")

# ============================================================================
# 7. VISUALIZE CALIBRATION CURVE
# ============================================================================

print("\n7. Creating calibration curve visualization...")

# Bin predictions and calculate empirical probabilities
bins = np.linspace(0, 1, 11)
calib_df['prob_bin'] = pd.cut(calib_df['prob_over_raw'], bins=bins, include_lowest=True)

# Calculate empirical frequencies
binned_raw = calib_df.groupby('prob_bin').agg({
    'actual_over': ['mean', 'count'],
    'prob_over_raw': 'mean',
    'prob_over_calibrated': 'mean'
}).reset_index()

binned_raw.columns = ['bin', 'empirical_freq', 'count', 'predicted_prob_raw', 'predicted_prob_calibrated']

# Plot
plt.figure(figsize=(10, 6))

# Perfect calibration line
plt.plot([0, 1], [0, 1], 'k--', label='Perfect calibration', linewidth=2)

# Raw model
plt.scatter(binned_raw['predicted_prob_raw'], binned_raw['empirical_freq'],
           s=binned_raw['count'], alpha=0.6, label='Raw model', color='red')

# Calibrated model
plt.scatter(binned_raw['predicted_prob_calibrated'], binned_raw['empirical_freq'],
           s=binned_raw['count'], alpha=0.6, label='Calibrated model', color='green')

plt.xlabel('Predicted Probability')
plt.ylabel('Empirical Frequency')
plt.title('Calibration Curve - Raw vs Calibrated Model')
plt.legend()
plt.grid(True, alpha=0.3)

plot_path = 'data/results/calibration_curve.png'
plt.savefig(plot_path, dpi=150, bbox_inches='tight')
print(f"✅ Calibration curve saved to {plot_path}")

# ============================================================================
# 8. SAVE CALIBRATED MODEL
# ============================================================================

print("\n8. Saving calibrated model...")

# Add calibrator to model dict
model_dict['calibrator'] = calibrator
model_dict['calibration_metrics'] = {
    'brier_score_raw': brier_raw,
    'brier_score_calibrated': brier_calibrated,
    'log_loss_raw': log_loss_raw,
    'log_loss_calibrated': log_loss_calibrated,
    'calibration_samples': len(calib_df)
}

# Save
calibrated_path = 'models/production_model_v2.0_CLEAN_CALIBRATED_latest.pkl'
with open(calibrated_path, 'wb') as f:
    pickle.dump(model_dict, f)

print(f"✅ Calibrated model saved to {calibrated_path}")

# ============================================================================
# SUMMARY
# ============================================================================

print("\n" + "="*80)
print("✅ CALIBRATION COMPLETE")
print("="*80)

print(f"\nCalibration Quality:")
print(f"  Brier Score: {brier_calibrated:.4f} (target: <0.15)")
print(f"  Log Loss: {log_loss_calibrated:.4f} (target: <0.50)")
print(f"  Samples: {len(calib_df):,}")

print(f"\nUsage in Betting:")
print(f"  1. Load calibrated model")
print(f"  2. Predict PRA for upcoming game")
print(f"  3. Convert to P(over) using logistic transform")
print(f"  4. Apply calibrator: P_calibrated = calibrator.predict(P_raw)")
print(f"  5. Calculate edge: P_calibrated - implied_probability")
print(f"  6. Bet if edge >= 5%")

print(f"\nResearch-backed benefit:")
print(f"  Calibrated models can improve ROI by 30-70% vs raw predictions")
print(f"  (Walsh & Joshi 2023: +34.69% ROI calibrated vs -35.17% raw)")

print("\n" + "="*80)
