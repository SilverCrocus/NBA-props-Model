"""
PHASE 1 Production Model Training - Feature Improvement Iteration

This version extends FIXED V2 with 139 Phase 1 features:
‚úÖ All FIXED V2 features (lag, CTG, contextual)
‚úÖ Advanced stats (TS%, USG%, pace-adjusted)
‚úÖ Enhanced opponent features (DvP, temporal opponent trends)
‚úÖ Consistency features (CV, volatility, boom/bust)
‚úÖ Recent form features (L3 averages, momentum, hot/cold)

Expected results:
- Training MAE: 5-7 points (improved from 6-8)
- Validation MAE: 6-8 points (improved from 7-9)
- Win rate: 54-55% (improved from 52.94%)

All features use .shift(1) for temporal isolation (no leakage).
"""

import pandas as pd
import numpy as np
from pathlib import Path
import xgboost as xgb
from sklearn.metrics import mean_absolute_error
import pickle
from datetime import datetime
import sys

# Add project root to path
sys.path.append(str(Path(__file__).parent.parent.parent))
sys.path.append('utils')

from ctg_feature_builder import CTGFeatureBuilder
from src.features.advanced_stats import AdvancedStatsFeatures
from src.features.opponent_features import OpponentFeatures
from src.features.consistency_features import ConsistencyFeatures
from src.features.recent_form_features import RecentFormFeatures

print("=" * 80)
print("PHASE 1 MODEL TRAINING - FEATURE IMPROVEMENT")
print("=" * 80)
print("\nüöÄ PHASE 1 FEATURES:")
print("  ‚úÖ FIXED V2 baseline (27 features)")
print("  ‚úÖ Advanced stats (TS%, USG%, pace-adjusted)")
print("  ‚úÖ Enhanced opponent features (DvP, temporal trends)")
print("  ‚úÖ Consistency metrics (CV, volatility, boom/bust)")
print("  ‚úÖ Recent form (L3 averages, momentum, hot/cold)")
print("  üìä Expected: 139 additional features")

# Initialize feature calculators
print("\n1. Initializing feature calculators...")
ctg_builder = CTGFeatureBuilder()
advanced_stats = AdvancedStatsFeatures()
opponent_features = OpponentFeatures()
consistency_features = ConsistencyFeatures()
recent_form = RecentFormFeatures()
print("   ‚úÖ All feature calculators initialized")

# Load combined game logs
print("\n2. Loading combined game logs (2003 - April 2025)...")
df = pd.read_csv('data/game_logs/all_game_logs_through_2025.csv')
df['GAME_DATE'] = pd.to_datetime(df['GAME_DATE'])
df = df.sort_values(['PLAYER_ID', 'GAME_DATE'])

print(f"‚úÖ Loaded {len(df):,} games")
print(f"   Date range: {df['GAME_DATE'].min()} to {df['GAME_DATE'].max()}")

# Add PRA if not present
if 'PRA' not in df.columns:
    df['PRA'] = df['PTS'] + df['REB'] + df['AST']

# Add SEASON if not present (required for some features)
if 'SEASON' not in df.columns:
    df['SEASON'] = df['GAME_DATE'].apply(lambda x: f"{x.year}-{str(x.year+1)[-2:]}")

# ======================================================================
# FILTER DNP/GARBAGE TIME GAMES
# ======================================================================
print("\n3. Filtering DNP/garbage time games...")
print(f"   Before: {len(df):,} games")

df = df[df['MIN'] >= 10].copy()

print(f"   After: {len(df):,} games")
print("   ‚úÖ Filtered DNP games")

# ======================================================================
# CALCULATE BASELINE LAG FEATURES (FIXED V2)
# ======================================================================
print("\n4. Calculating baseline lag features...")
player_groups = df.groupby('PLAYER_ID')

# Lag features
for lag in [1, 3, 5, 7, 10]:
    df[f'PRA_lag{lag}'] = player_groups['PRA'].shift(lag)
    print(f"   ‚úÖ PRA_lag{lag}")

# Rolling averages
for window in [3, 5, 10, 20]:
    df[f'PRA_L{window}_mean'] = (
        player_groups['PRA'].shift(1).rolling(window=window, min_periods=1).mean()
    )
    df[f'PRA_L{window}_std'] = (
        player_groups['PRA'].shift(1).rolling(window=window, min_periods=2).std()
    )
    print(f"   ‚úÖ L{window} rolling features")

# EWMA features
for span in [5, 10, 15]:
    df[f'PRA_ewma{span}'] = (
        player_groups['PRA'].shift(1).ewm(span=span, min_periods=1).mean()
    )
    print(f"   ‚úÖ EWMA{span}")

# Trend features
df['PRA_trend_L5_L20'] = df['PRA_L5_mean'] - df['PRA_L20_mean']

# ======================================================================
# ADD PRE-GAME CONTEXTUAL FEATURES (FIXED V2)
# ======================================================================
print(f"\n5. Adding pre-game contextual features...")

# Minutes Projected (L5 average of past games)
df['Minutes_Projected'] = player_groups['MIN'].shift(1).rolling(5, min_periods=1).mean()
print(f"   ‚úÖ Minutes_Projected (L5 average)")

# Days rest (days since last game)
df['Days_Since_Last_Game'] = df.groupby('PLAYER_ID')['GAME_DATE'].diff().dt.days
df['Days_Rest'] = df['Days_Since_Last_Game'].fillna(7).clip(upper=7)
print(f"   ‚úÖ Days_Rest")

# Back-to-back games
df['Is_BackToBack'] = (df['Days_Rest'] <= 1).astype(int)
print(f"   ‚úÖ Is_BackToBack")

# Games in last 7 days
def calculate_games_last_7(group):
    """For each game, count games in previous 7 days"""
    result = []
    for i, current_date in enumerate(group['GAME_DATE']):
        past_7_days = (group['GAME_DATE'] < current_date) & (group['GAME_DATE'] >= current_date - pd.Timedelta(days=7))
        result.append(past_7_days.sum())
    return pd.Series(result, index=group.index)

df['Games_Last7'] = df.groupby('PLAYER_ID').apply(calculate_games_last_7).reset_index(level=0, drop=True)
df['Games_Last7'] = df['Games_Last7'].fillna(0).astype(int)
print(f"   ‚úÖ Games_Last7")

baseline_feature_count = len([col for col in df.columns if any(x in col for x in
                             ['_lag', '_L3_', '_L5_', '_L10_', '_L20_', '_ewma', '_trend',
                              'Minutes_Projected', 'Days_Rest', 'Is_BackToBack', 'Games_Last7'])])
print(f"\n   Baseline features: {baseline_feature_count}")

# ======================================================================
# ADD CTG FEATURES
# ======================================================================
print(f"\n6. Adding CTG features (vectorized merge)...")

df['CTG_SEASON'] = df['SEASON'].apply(lambda x: x if '-' in str(x) else f"{x[:4]}-{x[4:6]}")

unique_player_seasons = df[['PLAYER_NAME', 'CTG_SEASON']].drop_duplicates()
print(f"   Unique player-seasons: {len(unique_player_seasons):,}")

print("   Loading CTG data...")
ctg_data = []
for idx, row in unique_player_seasons.iterrows():
    player_name = row['PLAYER_NAME']
    season = row['CTG_SEASON']

    ctg_feats = ctg_builder.get_player_ctg_features(player_name, season)
    ctg_feats['PLAYER_NAME'] = player_name
    ctg_feats['CTG_SEASON'] = season
    ctg_data.append(ctg_feats)

    if len(ctg_data) % 500 == 0:
        print(f"      Processed {len(ctg_data):,} / {len(unique_player_seasons):,} player-seasons...")

ctg_df = pd.DataFrame(ctg_data)
print(f"   ‚úÖ Loaded {len(ctg_df):,} player-season CTG records")

print("   Merging CTG features to game logs...")
df = df.merge(ctg_df, on=['PLAYER_NAME', 'CTG_SEASON'], how='left')

# Impute CTG features
ctg_features = ['CTG_USG', 'CTG_PSA', 'CTG_AST_PCT', 'CTG_TOV_PCT', 'CTG_eFG', 'CTG_REB_PCT']
ctg_defaults = {
    'CTG_USG': 0.20,
    'CTG_PSA': 1.10,
    'CTG_AST_PCT': 0.12,
    'CTG_TOV_PCT': 0.12,
    'CTG_eFG': 0.53,
    'CTG_REB_PCT': 0.10
}

for col, default_val in ctg_defaults.items():
    df[col] = df[col].fillna(default_val)

print(f"   ‚úÖ CTG features merged and imputed")

# ======================================================================
# ADD PHASE 1 FEATURES
# ======================================================================
print(f"\n7. Adding Phase 1 features (139 additional features)...")
print()

initial_col_count = len(df.columns)

# Advanced stats
df = advanced_stats.add_all_features(df)
print()

# Opponent features (skip if no MATCHUP column)
if 'MATCHUP' in df.columns:
    try:
        opponent_features.load_team_stats('2023-24')
        df = opponent_features.add_all_features(df)
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Skipping opponent features: {e}")
print()

# Consistency features
df = consistency_features.add_all_features(df)
print()

# Recent form features
df = recent_form.add_all_features(df)
print()

phase1_feature_count = len(df.columns) - initial_col_count
print(f"‚úÖ Added {phase1_feature_count} Phase 1 features")

# ======================================================================
# IMPUTE MISSING PHASE 1 FEATURES
# ======================================================================
print(f"\n8. Imputing missing Phase 1 feature values...")

# Get all Phase 1 feature columns
phase1_cols = [col for col in df.columns if any(x in col for x in
              ['TS_pct', 'USG_pct', 'per_100', 'pace', 'opp_', 'dvp_',
               '_CV_', '_std_', 'volatility', 'consistency', 'boom', 'bust',
               'floor', 'ceiling', '_L3_', 'momentum', 'hot', 'cold', 'streak',
               'trend', 'acceleration', 'role_change'])]

# Fill NaN with 0 (common for derived features)
for col in phase1_cols:
    if col in df.columns:
        missing_count = df[col].isna().sum()
        if missing_count > 0:
            df[col] = df[col].fillna(0)

print(f"   ‚úÖ Imputed {len(phase1_cols)} Phase 1 feature columns")

# ======================================================================
# DEFINE FULL FEATURE SET
# ======================================================================
print(f"\n9. Defining full feature set (baseline + Phase 1)...")

# Baseline features (FIXED V2)
lag_features = [col for col in df.columns if any(x in col for x in
                ['_lag', '_L3_', '_L5_', '_L10_', '_L20_', '_ewma', '_trend'])]

ctg_features_list = ['CTG_USG', 'CTG_PSA', 'CTG_AST_PCT', 'CTG_TOV_PCT', 'CTG_eFG', 'CTG_REB_PCT']

contextual_features = ['Minutes_Projected', 'Days_Rest', 'Is_BackToBack', 'Games_Last7']

# Phase 1 features
phase1_features = [col for col in df.columns if any(x in col for x in
                  ['TS_pct', 'USG_pct', 'per_100', 'pace_factor', 'pace_differential',
                   'opp_', 'dvp_', '_CV_', 'volatility', 'consistency_score', 'boom', 'bust',
                   'floor', 'ceiling', 'momentum', 'hot', 'cold', 'acceleration',
                   'role_change', 'FG_PCT_L3', 'FG3_PCT_L3', 'FT_PCT_L3'])]

# CRITICAL FIX: Remove leaked per_100 features (those without _L lag indicator)
# These use current game stats, which is data leakage!
leaked_per_100 = [col for col in phase1_features if 'per_100' in col and '_L' not in col]
phase1_features = [col for col in phase1_features if col not in leaked_per_100]

if leaked_per_100:
    print(f"\n   ‚ö†Ô∏è  REMOVED {len(leaked_per_100)} leaked per_100 features:")
    for feat in leaked_per_100:
        print(f"      - {feat}")

# Combine all features
feature_cols = lag_features + ctg_features_list + contextual_features + phase1_features
feature_cols = list(set([col for col in feature_cols if col in df.columns]))  # Remove duplicates

print(f"\n   Feature breakdown:")
print(f"      Baseline lag features: {len([c for c in lag_features if c in df.columns])}")
print(f"      CTG features: {len(ctg_features_list)}")
print(f"      Contextual features: {len(contextual_features)}")
print(f"      Phase 1 features: {len([c for c in phase1_features if c in df.columns])}")
print(f"      Total features: {len(feature_cols)}")

# Verify no in-game features
in_game_features = ['FGA', 'MIN', 'FG_PCT', 'FTA', 'FT_PCT', 'OREB', 'DREB', 'STL', 'BLK', 'TOV', 'PF']
found_in_game = [f for f in in_game_features if f in feature_cols]

if found_in_game:
    print(f"\n   ‚ö†Ô∏è  WARNING: Found in-game features: {found_in_game}")
    # Remove in-game features
    feature_cols = [f for f in feature_cols if f not in in_game_features]
    print(f"   ‚úÖ Removed in-game features. New total: {len(feature_cols)}")
else:
    print(f"\n   ‚úÖ NO in-game features found (correct!)")

# ======================================================================
# CREATE TEMPORAL SPLITS
# ======================================================================
print(f"\n10. Creating temporal train/val/test splits...")

# Only require PRA to be non-null
valid_mask = df['PRA'].notna() & (df['PRA'] > 0)
df_clean = df[valid_mask].copy()

zero_pra_count = len(df) - len(df_clean)
print(f"   Dropped {zero_pra_count:,} rows with PRA <= 0 ({zero_pra_count/len(df)*100:.1f}%)")
print(f"   Retained {len(df_clean):,} games ({len(df_clean)/len(df)*100:.1f}%)")

# Fill any remaining NaN in feature columns with 0
df_clean[feature_cols] = df_clean[feature_cols].fillna(0)

# Temporal splits
train_df = df_clean[df_clean['GAME_DATE'] <= '2023-06-30'].copy()
val_df = df_clean[(df_clean['GAME_DATE'] > '2023-06-30') &
                   (df_clean['GAME_DATE'] <= '2024-06-30')].copy()
test_df = df_clean[df_clean['GAME_DATE'] > '2024-06-30'].copy()

print(f"\n   Train set: {len(train_df):,} games ({train_df['GAME_DATE'].min()} to {train_df['GAME_DATE'].max()})")
print(f"   Val set:   {len(val_df):,} games ({val_df['GAME_DATE'].min()} to {val_df['GAME_DATE'].max()})")
print(f"   Test set:  {len(test_df):,} games ({test_df['GAME_DATE'].min()} to {test_df['GAME_DATE'].max()})")

X_train = train_df[feature_cols]
y_train = train_df['PRA']

X_val = val_df[feature_cols]
y_val = val_df['PRA']

X_test = test_df[feature_cols]
y_test = test_df['PRA']

# ======================================================================
# TRAIN XGBOOST WITH REGULARIZATION
# ======================================================================
print(f"\n11. Training XGBoost (baseline + Phase 1 features)...")

model = xgb.XGBRegressor(
    objective='reg:gamma',
    n_estimators=1000,
    max_depth=4,
    learning_rate=0.01,
    subsample=0.8,
    colsample_bytree=0.8,
    min_child_weight=10,
    gamma=0.1,
    reg_alpha=0.1,
    reg_lambda=5,
    random_state=42,
    n_jobs=-1,
    verbosity=1
)

print("\n   Training with early stopping...")
model.fit(
    X_train, y_train,
    eval_set=[(X_train, y_train), (X_val, y_val)],
    verbose=100
)

print(f"\n‚úÖ Model trained")

if hasattr(model, 'best_iteration') and model.best_iteration is not None:
    print(f"   Best iteration: {model.best_iteration}")
    print(f"   Trees used: {model.best_iteration} / {model.n_estimators}")
else:
    print(f"   Trees used: {model.n_estimators} (no early stopping triggered)")

# ======================================================================
# EVALUATE PERFORMANCE
# ======================================================================
print(f"\n12. Evaluating model performance...")

train_pred = model.predict(X_train)
train_mae = mean_absolute_error(y_train, train_pred)

val_pred = model.predict(X_val)
val_mae = mean_absolute_error(y_val, val_pred)

test_pred = model.predict(X_test)
test_mae = mean_absolute_error(y_test, test_pred)

print(f"\n   Training MAE:   {train_mae:.2f} points")
print(f"   Validation MAE: {val_mae:.2f} points")
print(f"   Test MAE:       {test_mae:.2f} points")

train_val_gap = abs(train_mae - val_mae) / val_mae * 100
print(f"\n   Train/Val gap: {train_val_gap:.1f}% (should be <20%)")

# Check for negative predictions
neg_train = (train_pred < 0).sum()
neg_val = (val_pred < 0).sum()
neg_test = (test_pred < 0).sum()

print(f"\n   Negative predictions:")
print(f"      Train: {neg_train} / {len(train_pred)} ({neg_train/len(train_pred)*100:.2f}%)")
print(f"      Val:   {neg_val} / {len(val_pred)} ({neg_val/len(val_pred)*100:.2f}%)")
print(f"      Test:  {neg_test} / {len(test_pred)} ({neg_test/len(test_pred)*100:.2f}%)")

# ======================================================================
# FEATURE IMPORTANCE
# ======================================================================
print(f"\n13. Feature importance analysis...")

importance_df = pd.DataFrame({
    'feature': feature_cols,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

print(f"\n   Top 20 most important features:")
print(importance_df.head(20).to_string(index=False))

# Check Phase 1 feature importance
phase1_importance = importance_df[importance_df['feature'].isin(phase1_features)]
print(f"\n   Phase 1 features in top 20: {len(phase1_importance[phase1_importance.index < 20])}")

# ======================================================================
# SAVE MODEL
# ======================================================================
print(f"\n14. Saving model...")

timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
model_path = f'models/production_model_PHASE1_{timestamp}.pkl'

model_dict = {
    'model': model,
    'feature_cols': feature_cols,
    'train_mae': train_mae,
    'val_mae': val_mae,
    'test_mae': test_mae,
    'training_samples': len(train_df),
    'val_samples': len(val_df),
    'test_samples': len(test_df),
    'date_range': f"{df_clean['GAME_DATE'].min()} to {df_clean['GAME_DATE'].max()}",
    'feature_importance': importance_df,
    'hyperparameters': model.get_params(),
    'version': 'PHASE1',
    'phase1_features': phase1_features,
    'phase1_feature_count': len([c for c in phase1_features if c in df.columns]),
    'fixes_applied': [
        'PRE-GAME FEATURES ONLY (no FGA, MIN, FG_PCT)',
        'Added Minutes_Projected',
        'Added contextual features (Days_Rest, Is_BackToBack)',
        'DNP game filtering (MIN >= 10)',
        'Data imputation',
        'Train/val/test splits',
        'XGBoost regularization',
        'Non-negative predictions (reg:gamma)',
        'PHASE 1: Advanced stats (TS%, USG%, pace-adjusted)',
        'PHASE 1: Enhanced opponent features (DvP, temporal trends)',
        'PHASE 1: Consistency features (CV, volatility, boom/bust)',
        'PHASE 1: Recent form features (L3 averages, momentum, hot/cold)'
    ],
    'timestamp': timestamp
}

with open(model_path, 'wb') as f:
    pickle.dump(model_dict, f)

print(f"‚úÖ Model saved to {model_path}")

# Also save as latest
latest_path = 'models/production_model_PHASE1_latest.pkl'
with open(latest_path, 'wb') as f:
    pickle.dump(model_dict, f)

print(f"‚úÖ Model saved to {latest_path}")

# Save feature importance
importance_path = f'models/feature_importance_PHASE1_{timestamp}.csv'
importance_df.to_csv(importance_path, index=False)
print(f"‚úÖ Feature importance saved to {importance_path}")

# ======================================================================
# VALIDATION SUMMARY
# ======================================================================
print("\n" + "=" * 80)
print("PHASE 1 MODEL TRAINING COMPLETE - VALIDATION SUMMARY")
print("=" * 80)

checks_passed = []
checks_failed = []

# Check 1: Training MAE should be 5-7 (improved from 6-8)
if 5 <= train_mae <= 7:
    checks_passed.append(f"‚úÖ Training MAE: {train_mae:.2f} (improved)")
elif 6 <= train_mae <= 8:
    checks_passed.append(f"‚úÖ Training MAE: {train_mae:.2f} (baseline)")
else:
    checks_failed.append(f"‚ùå Training MAE: {train_mae:.2f} (expected 5-7)")

# Check 2: Validation MAE should be 6-8 (improved from 7-9)
if 6 <= val_mae <= 8:
    checks_passed.append(f"‚úÖ Validation MAE: {val_mae:.2f} (improved)")
elif 7 <= val_mae <= 9:
    checks_passed.append(f"‚úÖ Validation MAE: {val_mae:.2f} (baseline)")
else:
    checks_failed.append(f"‚ùå Validation MAE: {val_mae:.2f} (expected 6-8)")

# Check 3: Test MAE should be 7-9 (improved from 8-10)
if 7 <= test_mae <= 9:
    checks_passed.append(f"‚úÖ Test MAE: {test_mae:.2f} (improved)")
elif 8 <= test_mae <= 10:
    checks_passed.append(f"‚úÖ Test MAE: {test_mae:.2f} (baseline)")
else:
    checks_failed.append(f"‚ö†Ô∏è  Test MAE: {test_mae:.2f} (expected 7-9)")

# Check 4: Train/Val gap should be <20%
if train_val_gap < 20:
    checks_passed.append(f"‚úÖ Train/Val gap: {train_val_gap:.1f}% (healthy)")
else:
    checks_failed.append(f"‚ùå Train/Val gap: {train_val_gap:.1f}% (expected <20%)")

# Check 5: No negative predictions
if neg_train == 0 and neg_val == 0 and neg_test == 0:
    checks_passed.append(f"‚úÖ No negative predictions")
else:
    checks_failed.append(f"‚ùå Negative predictions found")

# Check 6: Phase 1 features added
phase1_count = len([c for c in phase1_features if c in df.columns])
if phase1_count >= 100:
    checks_passed.append(f"‚úÖ Phase 1 features: {phase1_count} (target: 139)")
else:
    checks_failed.append(f"‚ùå Phase 1 features: {phase1_count} (expected 139)")

# Check 7: Data retention
data_retention = len(df_clean) / len(df) * 100
if data_retention >= 90:
    checks_passed.append(f"‚úÖ Data retention: {data_retention:.1f}%")
else:
    checks_failed.append(f"‚ùå Data retention: {data_retention:.1f}%")

print(f"\nüìä VALIDATION CHECKS:")
for check in checks_passed:
    print(f"   {check}")
for check in checks_failed:
    print(f"   {check}")

print(f"\nüìà PERFORMANCE SUMMARY:")
print(f"   Training MAE: {train_mae:.2f} points")
print(f"   Validation MAE: {val_mae:.2f} points")
print(f"   Test MAE: {test_mae:.2f} points")
print(f"   Total features: {len(feature_cols)}")
print(f"   Phase 1 features: {phase1_count}")
print(f"   Data retention: {data_retention:.1f}%")

if len(checks_failed) == 0:
    print(f"\nüéâ ALL VALIDATION CHECKS PASSED!")
    print(f"   Model is ready for walk-forward validation.")
else:
    print(f"\n‚ö†Ô∏è  {len(checks_failed)} validation check(s) failed/warnings.")

print("\n" + "=" * 80)
print("Next step: Run walk-forward validation on 2024-25 to validate win rate")
print("Expected: 54-55% win rate (improved from 52.94%)")
print("=" * 80)
